{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c8447c-afb5-46b5-9b44-023332d5d296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 02:01:49.628764: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-03 02:01:49.657729: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-03 02:01:49.657742: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-03 02:01:49.658382: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-03 02:01:49.662618: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-03 02:01:50.214524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabled dumping callback in thread MainThread (dump root: /workspace/logs/run_02_03_02_01/debug, tensor debug mode: FULL_HEALTH)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.debug.lib.debug_events_writer.DebugEventsWriter at 0x7f36e7aaba00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Block 1\n",
    "# Imports\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tokenize\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorboard\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "base_log_dir = os.path.join(\"/workspace/logs\", \"run_\" + datetime.datetime.now().strftime(\"%m_%d_%H_%M\"))\n",
    "fit_log_dir = os.path.join(base_log_dir, \"fit\")\n",
    "loader_log_dir = os.path.join(base_log_dir, \"loader\")\n",
    "debug_log_dir = os.path.join(base_log_dir, \"debug\")\n",
    "\n",
    "# Enable full debugging\n",
    "#tf.config.optimizer.set_jit(False)  # Disable XLA compilation\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.debugging.experimental.enable_dump_debug_info(\n",
    "    debug_log_dir,\n",
    "    tensor_debug_mode=\"FULL_HEALTH\", # CONCISE_HEALTH\n",
    "    circular_buffer_size=-1) # 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd3532f1-59c9-4421-83cb-49c9dd54bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2\n",
    "# Loader\n",
    "\n",
    "\n",
    "class Loader:\n",
    "    def __init__(self, problems_path, submissions_dir, loader_log_dir, max_length_input, max_length_output, batch_size):\n",
    "        self.problems_path = problems_path\n",
    "        self.submissions_dir = submissions_dir\n",
    "        self.max_length_input = max_length_input\n",
    "        self.max_length_output = max_length_output\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.problem_tokenizer = Tokenizer(filters='')\n",
    "        self.solution_tokenizer = Tokenizer(filters='', oov_token='UNK')\n",
    "        self.dataset = None\n",
    "        \n",
    "        self.writer = tf.summary.create_file_writer(loader_log_dir)\n",
    "\n",
    "    def _load_problems_and_solutions(self):\n",
    "        # Load problems\n",
    "        with open(self.problems_path, 'r') as problems_file:\n",
    "            problems_list = json.load(problems_file)\n",
    "        raw_problems = {}\n",
    "        \n",
    "        for problem in problems_list:\n",
    "            problem_id = problem['problem_id']\n",
    "            concatenated_problem = \"XXSTATEMENT {} XXINPUT {} XXOUTPUT {} XXNOTES {} XXEXAMPLES {}\".format(\n",
    "                problem.get('problem_statement', ''),\n",
    "                problem.get('problem_input', ''),\n",
    "                problem.get('problem_output', ''),\n",
    "                problem.get('problem_notes', ''),\n",
    "                problem.get('examples', '')\n",
    "            )\n",
    "            raw_problems[problem_id] = concatenated_problem\n",
    "            \n",
    "        # Load solutions\n",
    "        raw_solutions = [[] for _ in range(len(raw_problems) * 2)] # Estimate allows us to delete up to half\n",
    "        submissions = glob.glob(os.path.join(self.submissions_dir, \"*.py\"))\n",
    "        \n",
    "        for submission_path in submissions:\n",
    "            problem_number = int(re.findall(r'^\\d+', os.path.basename(submission_path))[0])\n",
    "            with open(submission_path, \"r\") as f:\n",
    "                solutionList = []\n",
    "                for token in tokenize.generate_tokens(f.readline):\n",
    "                    solutionList.append(token.string)\n",
    "                raw_solutions[problem_number].append(solutionList)\n",
    "        \n",
    "        # Flatten problems and solutions\n",
    "        problems = []\n",
    "        solutions = []\n",
    "        for problem_id, solution_set in enumerate(raw_solutions):\n",
    "            if solution_set:\n",
    "                for solution in solution_set:\n",
    "                    problems.append(raw_problems[problem_id])\n",
    "                    solutions.append(solution)\n",
    "\n",
    "        return problems, solutions\n",
    "\n",
    "    def tokenize_and_pad(self, problems, solutions):\n",
    "        self.problem_tokenizer.fit_on_texts(problems)\n",
    "        self.solution_tokenizer.fit_on_texts(solutions)\n",
    "\n",
    "        problem_sequences = self.problem_tokenizer.texts_to_sequences(problems)\n",
    "        solution_sequences = self.solution_tokenizer.texts_to_sequences(solutions)\n",
    "\n",
    "        problems_padded = pad_sequences(problem_sequences, padding='post', maxlen=self.max_length_input)\n",
    "        solutions_padded = pad_sequences(solution_sequences, padding='post', maxlen=self.max_length_output)\n",
    "        \n",
    "        return problems_padded, solutions_padded\n",
    "    \n",
    "    def log_vocabulary(self):\n",
    "        with self.writer.as_default():\n",
    "            for word, index in self.problem_tokenizer.word_index.items():\n",
    "                tf.summary.text(name=\"Problem Vocabulary\", data=f\"{word}: {index}\", step=0)\n",
    "\n",
    "            for word, index in self.solution_tokenizer.word_index.items():\n",
    "                tf.summary.text(name=\"Solution Vocabulary\", data=f\"{word}: {index}\", step=0)\n",
    "\n",
    "            self.writer.flush()\n",
    "    \n",
    "    def _create_tf_dataset(self, problem_padded, solution_padded):\n",
    "        # Prepare decoder input (shifted solution)\n",
    "        decoder_input = tf.pad(solution_padded, [[0, 0], [1, 0]])[:, :-1]  # Shift left\n",
    "\n",
    "        # Ground truth\n",
    "        target = solution_padded\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(((problem_padded, decoder_input), target))\n",
    "        \n",
    "        return dataset.shuffle(buffer_size=1024).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    def _log_dataset_samples(self, problem_padded, solution_padded):\n",
    "        with self.writer.as_default():\n",
    "            for i, (problem, solution) in enumerate(zip(problem_padded, solution_padded)):\n",
    "                if i >= 5:  # Log 5 samples\n",
    "                    break\n",
    "                \n",
    "                # Convert padded sequences back to text\n",
    "                problem_text = self.problem_tokenizer.sequences_to_texts([problem])\n",
    "                solution_text = self.solution_tokenizer.sequences_to_texts([solution])\n",
    "\n",
    "                # Truncate texts\n",
    "                #max_display_length = 1024\n",
    "                #problem_text = (problem_text[:max_display_length] + '...') if len(problem_text) > max_display_length else problem_text\n",
    "                #solution_text = (solution_text[:max_display_length] + '...') if len(solution_text) > max_display_length else solution_text\n",
    "\n",
    "                # Log to TensorBoard\n",
    "                tf.summary.text(name=f\"Problem_{i}\", data=problem_text, step=0)\n",
    "                tf.summary.text(name=f\"Solution_{i}\", data=solution_text, step=0)\n",
    "\n",
    "            self.writer.flush()\n",
    "            \n",
    "    def load_data(self):\n",
    "        problems, solutions = self._load_problems_and_solutions()\n",
    "\n",
    "        problems_padded, solutions_padded = self.tokenize_and_pad(problems, solutions)\n",
    "        self._log_dataset_samples(problems_padded, solutions_padded)\n",
    "        self.dataset = self._create_tf_dataset(problems_padded, solutions_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd0ef91d-0806-4046-a111-f1916d98f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3\n",
    "# Positional Encoder\n",
    "\n",
    "def positional_encoder(seq_length, dim):\n",
    "    # Generate positions for each element\n",
    "    positions = tf.range(seq_length, dtype=tf.float32)[..., tf.newaxis]\n",
    "\n",
    "    # Create a range for the dimensions and compute division terms\n",
    "    # Uses geometric progression with a base of 10,000; the rate depends on the dimension\n",
    "    i = tf.range(dim, dtype=tf.float32)\n",
    "    div_terms = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(dim, tf.float32))\n",
    "\n",
    "    # Calculate odd/even sinusoidal encodings\n",
    "    angle_rates = positions * div_terms\n",
    "    sine = tf.sin(angle_rates[:, 0::2])\n",
    "    cosine = tf.cos(angle_rates[:, 1::2])\n",
    "\n",
    "    # Interlace and reshape\n",
    "    pos_encoding = tf.reshape(tf.concat([sine, cosine], axis=-1), [1, seq_length, dim])\n",
    "\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d505a2d-a02c-4bc5-9ca5-7555f9418fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4\n",
    "# Encoder/Decoder Layer classes\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, dim_ff, key_dim, num_heads, dropout_rate, name=\"EncoderLayer\"):\n",
    "        super(EncoderLayer, self).__init__(name=name)\n",
    "\n",
    "        # Multi-Head Self-Attention layer\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "\n",
    "        # Feed-Forward Network Layers\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim_ff, kernel_initializer='he_normal', name=\"encoder_ffn_dense1\"), \n",
    "            tf.keras.layers.LeakyReLU(alpha=0.01), # Trying LeakyReLu\n",
    "            tf.keras.layers.Dense(dim, kernel_initializer='he_normal', name=\"encoder_ffn_dense2\")\n",
    "        ], name=\"encoder_ffn\")\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"encoder_layernorm1\")\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"encoder_layernorm2\")\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_mha = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # Self-Attention\n",
    "        attn_output = self.mha(x, x)\n",
    "        attn_output = self.dropout_mha(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # Residual connection\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection\n",
    "\n",
    "        return out2\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(EncoderLayer, self).get_config()\n",
    "        mha_config = self.mha.get_config()\n",
    "        config.update({\n",
    "            \"dim\": self.ffn.layers[2].units, \n",
    "            \"dim_ff\": self.ffn.layers[0].units, \n",
    "            \"num_heads\": mha_config['num_heads'], \n",
    "            \"key_dim\": mha_config['key_dim'], \n",
    "            \"dropout_rate\": self.dropout_mha.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, dim_ff, key_dim, num_heads, dropout_rate, name=\"DecoderLayer\"):\n",
    "        super(DecoderLayer, self).__init__(name=name)\n",
    "\n",
    "        # Self-Attention and Cross-Attention layers\n",
    "        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "\n",
    "        # Feed Forward Network Layers\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim_ff, kernel_initializer='he_normal', name=\"decoder_ffn_dense1\"), \n",
    "            tf.keras.layers.LeakyReLU(alpha=0.01), # Trying LeakyReLu\n",
    "            tf.keras.layers.Dense(dim, kernel_initializer='he_normal', name=\"decoder_ffn_dense2\")\n",
    "        ], name=\"decoder_ffn\")\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm1\")\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm2\")\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm3\")\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_self_attn = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout_cross_attn = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        # Self-Attention\n",
    "        attn1_output = self.mha1(x, x, attention_mask=look_ahead_mask)\n",
    "        attn1_output = self.dropout_self_attn(attn1_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn1_output)  # Residual connection\n",
    "\n",
    "        # Cross-Attention\n",
    "        attn2_output = self.mha2(out1, enc_output, attention_mask=padding_mask)\n",
    "        attn2_output = self.dropout_cross_attn(attn2_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + attn2_output)  # Residual connection\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # Residual connection\n",
    "\n",
    "        return out3\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DecoderLayer, self).get_config()\n",
    "        mha_config = self.mha1.get_config()  # Assumes mha1 and mha2 have the same configuration\n",
    "        config.update({\n",
    "            'dim': self.ffn.layers[2].units,\n",
    "            'dim_ff': self.ffn.layers[0].units,\n",
    "            'num_heads': mha_config['num_heads'],\n",
    "            'key_dim': mha_config['key_dim'],\n",
    "            'dropout_rate': self.dropout_self_attn.rate\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bcb9091-5ff0-4c54-89c8-ee0a5463416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5\n",
    "# Transformer\n",
    "\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"TransformerEncoder\"):\n",
    "        super(TransformerEncoder, self).__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [EncoderLayer(dim, dim_ff, key_dim, num_heads, dropout_rate, name=f\"encoder_layer_{i}\") for i in range(num_layers)]\n",
    "\n",
    "    def call(self, x: tf.Tensor, training=False) -> tf.Tensor:\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"TransformerDecoder\"):\n",
    "        super(TransformerDecoder, self).__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.dec_layers = [DecoderLayer(dim, dim_ff, key_dim, num_heads, dropout_rate, name=f\"decoder_layer_{i}\") for i in range(num_layers)]\n",
    "\n",
    "    def call(self, x: tf.Tensor, enc_output: tf.Tensor, training=False) -> tf.Tensor:\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, enc_output, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, dim, dim_ff, key_dim, problem_vocab_size, solution_vocab_size, num_heads, num_layers, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Separate embedding for input and output\n",
    "        self.problem_embedding_layer = tf.keras.layers.Embedding(problem_vocab_size, dim)\n",
    "        self.solution_embedding_layer = tf.keras.layers.Embedding(solution_vocab_size, dim)\n",
    "\n",
    "        self.encoder = TransformerEncoder(dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"encoder\")\n",
    "        self.decoder = TransformerDecoder(dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"decoder\")\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(solution_vocab_size, name=\"output_layer\")\n",
    "\n",
    "    def call(self, encoder_input, decoder_input, training=False):\n",
    "        encoder_emb = self.problem_embedding_layer(encoder_input)\n",
    "        decoder_emb = self.solution_embedding_layer(decoder_input)\n",
    "\n",
    "        seq_length_enc = tf.shape(encoder_input)[1]\n",
    "        seq_length_dec = tf.shape(decoder_input)[1]\n",
    "        pos_encoding_enc = positional_encoder(seq_length_enc, self.dim)\n",
    "        pos_encoding_dec = positional_encoder(seq_length_dec, self.dim)\n",
    "\n",
    "        encoder_emb += pos_encoding_enc\n",
    "        decoder_emb += pos_encoding_dec\n",
    "\n",
    "        encoder_output = self.encoder(encoder_emb, training=training)\n",
    "        decoder_output = self.decoder(decoder_emb, encoder_output, training=training)\n",
    "\n",
    "        final_output = self.final_layer(decoder_output)\n",
    "\n",
    "        return final_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc04145d-725e-4db0-a9a6-ab2682e60890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6\n",
    "# Build and Compile\n",
    "\n",
    "def build_and_compile(dim, dim_ff, key_dim, nhead, num_layers, problem_vocab_size, solution_vocab_size, dropout_rate, learning_rate=1e-4):\n",
    "    # Define model inputs\n",
    "    encoder_input = tf.keras.Input(shape=(None,), dtype='int32', name='encoder_input')\n",
    "    decoder_input = tf.keras.Input(shape=(None,), dtype='int32', name='decoder_input')\n",
    "\n",
    "    # Initialize and call the Transformer\n",
    "    transformer = Transformer(dim, dim_ff, key_dim, problem_vocab_size, solution_vocab_size, nhead, num_layers, dropout_rate)\n",
    "    final_output = transformer(encoder_input, decoder_input)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=final_output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c79e4070-2bbe-423d-8228-4e9aa08347d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7\n",
    "# Define Training Steps\n",
    "\n",
    "def calculate_loss(model_output, tokenized_code, mask):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(tokenized_code, model_output, from_logits=True)\n",
    "    loss *= mask  # Apply mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, tokenized_question, tokenized_code, clip_norm=10.0):\n",
    "    with tf.GradientTape() as tape:\n",
    "        model_output = model([tokenized_question, tokenized_code], training=True)\n",
    "\n",
    "        # Mask PAD tokens\n",
    "        mask = tf.cast(tf.math.logical_not(tf.math.equal(tokenized_code, 0)), dtype=model_output.dtype)\n",
    "        \n",
    "        # Calculate loss\n",
    "        average_loss = calculate_loss(model_output, tokenized_code, mask)\n",
    "\n",
    "    # Compute and clip gradients\n",
    "    gradients = tape.gradient(average_loss, model.trainable_variables)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm)\n",
    "\n",
    "    # Apply gradients to update model weights\n",
    "    optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d03c601c-7456-48fe-8bbc-bbef9a8a469d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Failed to read source code from path: /tmp/ipykernel_103/107523660.py. Reason: Source path neither exists nor can be loaded as a .par file: /tmp/ipykernel_103/107523660.py\n",
      "WARNING:tensorflow:Failed to read source code from path: /tmp/ipykernel_103/1472765510.py. Reason: Source path neither exists nor can be loaded as a .par file: /tmp/ipykernel_103/1472765510.py\n",
      "WARNING:tensorflow:Failed to read source code from path: /tmp/ipykernel_103/1249505967.py. Reason: Source path neither exists nor can be loaded as a .par file: /tmp/ipykernel_103/1249505967.py\n",
      "WARNING:tensorflow:Failed to read source code from path: /tmp/ipykernel_103/509590977.py. Reason: Source path neither exists nor can be loaded as a .par file: /tmp/ipykernel_103/509590977.py\n",
      "WARNING:tensorflow:Failed to read source code from path: /tmp/ipykernel_103/1223476310.py. Reason: Source path neither exists nor can be loaded as a .par file: /tmp/ipykernel_103/1223476310.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 02:01:51.727392: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-03 02:01:51.772083: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Failed to read source code from path: /tmp/ipykernel_103/2549963760.py. Reason: Source path neither exists nor can be loaded as a .par file: /tmp/ipykernel_103/2549963760.py\n",
      "1/1 [==============================] - 32s 32s/step - loss: 10.9578 - accuracy: 0.0150\n",
      "INFO:tensorflow:Assets written to: /workspace/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /workspace/assets\n"
     ]
    }
   ],
   "source": [
    "# Block 8\n",
    "# Execution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to data\n",
    "    problems_path = \"/workspace/Training_Data/A_Problems.json\"\n",
    "    submissions_dir = \"/workspace/Training_Data/A_Submissions_4\"\n",
    "\n",
    "    #problems_path = \"C:/AIClub/Training_Data_Cleaned/A_Problems.json\"\n",
    "    #submissions_dir = \"C:/AIClub/Training_Data_Cleaned/A_Submissions\"\n",
    "    #You'll need to update it in Block 1\n",
    "\n",
    "    # Set hyperparameters\n",
    "    dim = 256\n",
    "    dim_ff = dim * 4\n",
    "    num_layers = 8\n",
    "    num_heads = 8\n",
    "    key_dim = dim // num_heads\n",
    "\n",
    "    max_length_input = 530 # Set to cover about 85% of inputs\n",
    "    max_length_output = 50 # Set to cover 100% of ground truth outputs\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "    batch_size = 32 # Raised to 32\n",
    "    learning_rate = 0.001\n",
    "    epochs = 1\n",
    "\n",
    "    assert dim % num_heads == 0, \"dim % num_heads != 0\"\n",
    "\n",
    "    # Initialize the Loader\n",
    "    loader = Loader(problems_path, submissions_dir, loader_log_dir, max_length_input, max_length_output, batch_size)\n",
    "    loader.load_data()\n",
    "    problem_vocab_size = len(loader.problem_tokenizer.word_index) + 1\n",
    "    solution_vocab_size = len(loader.solution_tokenizer.word_index) + 1\n",
    "    \n",
    "    # Build the model\n",
    "    model = build_and_compile(dim, dim_ff, key_dim, num_heads, num_layers, problem_vocab_size, solution_vocab_size, dropout_rate, learning_rate)\n",
    "\n",
    "    # Setup TensorBoard callback\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=fit_log_dir, histogram_freq=1)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(loader.dataset, epochs=epochs, callbacks=[tensorboard_callback]) # history variable unused...\n",
    "    \n",
    "    \"\"\"\n",
    "    Manual training setup\n",
    "    \n",
    "    optimizer = model.optimizer\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Start of Epoch {epoch+1}\")\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, ((tokenized_question, tokenized_code), target) in enumerate(loader.dataset):\n",
    "            # Call the custom train_step\n",
    "            loss = train_step(model, optimizer, tokenized_question, tokenized_code[:, :-1])\n",
    "\n",
    "            # Log every 200 batches\n",
    "            if step % 200 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        print(f\"End of Epoch {epoch+1}, Loss: {loss.numpy()}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(\"/workspace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "112669f0-144f-4c82-ae0d-9d5f93ee4fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Failed to read source code from path: /tmp/ipykernel_103/4165982196.py. Reason: Source path neither exists nor can be loaded as a .par file: /tmp/ipykernel_103/4165982196.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Failed to read source code from path: /tmp/ipykernel_103/4165982196.py. Reason: Source path neither exists nor can be loaded as a .par file: /tmp/ipykernel_103/4165982196.py\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Sample 1:\n",
      "Predicted sequence: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Predicted text: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Sample 2:\n",
      "Predicted sequence: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Predicted text: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Sample 3:\n",
      "Predicted sequence: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Predicted text: UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n"
     ]
    }
   ],
   "source": [
    "# Block 10\n",
    "# Evaluation Class\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, loader):\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "\n",
    "    def plot_loss(self, history):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.title('Loss Curve')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_token_probabilities(self, token_index, n_samples=1):\n",
    "        # Take one batch from the dataset\n",
    "        for (encoder_input, decoder_input), _ in self.loader.dataset.take(1):\n",
    "            # Slice the batch down to n_samples\n",
    "            encoder_input = encoder_input[:n_samples]\n",
    "            decoder_input = decoder_input[:n_samples]\n",
    "\n",
    "            # Predict on the sliced inputs\n",
    "            predictions = self.model.predict([encoder_input, decoder_input])\n",
    "\n",
    "            for sample_idx in range(n_samples):\n",
    "                # For each sample, extract token logits and convert to probabilities\n",
    "                token_logits = predictions[sample_idx, token_index, :]\n",
    "                token_probabilities = tf.nn.softmax(token_logits).numpy()\n",
    "                sorted_indices = np.argsort(token_probabilities)[::-1]\n",
    "                sorted_probabilities = token_probabilities[sorted_indices]\n",
    "\n",
    "                # Plotting\n",
    "                plt.figure(figsize=(20, 5))\n",
    "                plt.bar(range(len(sorted_probabilities)), sorted_probabilities)\n",
    "                plt.xlabel('Word Indices (sorted by probability)')\n",
    "                plt.ylabel('Probability')\n",
    "                plt.title(f'Word Prediction Probabilities for Token {token_index} in Sample {sample_idx+1}')\n",
    "                plt.show()\n",
    "\n",
    "    def generate_training_predictions(self, n_samples=1):\n",
    "        # To store prediction texts\n",
    "        predictions_texts = []\n",
    "\n",
    "        # Take one batch from the dataset\n",
    "        for (encoder_inputs, decoder_inputs), _ in self.loader.dataset.take(1):\n",
    "            # Slice the batch down to n_samples\n",
    "            encoder_inputs = encoder_inputs[:n_samples]\n",
    "            decoder_inputs = decoder_inputs[:n_samples]\n",
    "\n",
    "            # Predict on the sliced inputs\n",
    "            predictions = self.model.predict([encoder_inputs, decoder_inputs])\n",
    "            predicted_sequences = np.argmax(predictions, axis=-1)\n",
    "\n",
    "            # Convert sequences to text\n",
    "            predicted_texts = self.loader.solution_tokenizer.sequences_to_texts(predicted_sequences)\n",
    "\n",
    "            # Print each prediction in the slice\n",
    "            for i, predicted_text in enumerate(predicted_texts):\n",
    "                print(f\"Sample {i + 1}:\")\n",
    "                print(\"Predicted sequence:\", predicted_sequences[i])\n",
    "                print(\"Predicted text:\", predicted_text)\n",
    "    \n",
    "    def generate_manual_predictions(self, input_text):\n",
    "        # Tokenize the input string\n",
    "        input_seq = self.loader.problem_tokenizer.texts_to_sequences([input_text])\n",
    "        input_padded = tf.keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=self.loader.max_length_input, padding='post')\n",
    "\n",
    "        # Prepare the decoder input\n",
    "        start_token_index = self.loader.solution_tokenizer.word_index.get('[START]', 1)  # Fallback to 1 if not found\n",
    "        decoder_input = np.array([[start_token_index]])\n",
    "        \n",
    "        # Generate and interpret the prediction\n",
    "        predictions = self.model.predict([input_padded, decoder_input])\n",
    "        predicted_sequence = np.argmax(predictions, axis=-1)[0]\n",
    "        predicted_text = self.loader.solution_tokenizer.sequences_to_texts([predicted_sequence])\n",
    "        \n",
    "        # Print the output\n",
    "        print(\"Input text:\", input_text)\n",
    "        print(\"Predicted text:\", predicted_text[0])\n",
    "\n",
    "    def evaluate(self, command, *args, **kwargs):\n",
    "        if command == 'loss':\n",
    "            self.plot_loss(*args, **kwargs)\n",
    "        elif command == 'token_prob':\n",
    "            self.plot_token_probabilities(*args, **kwargs)\n",
    "        elif command == 'training_sample_pred':\n",
    "            self.generate_training_predictions(*args, **kwargs)\n",
    "        elif command == 'manual_sample_pred':\n",
    "            self.generate_manual_predictions(*args, **kwargs)\n",
    "        else:\n",
    "            print(f\"Unknown command: {command}\")\n",
    "\n",
    "# Load the model if it's not\n",
    "model = tf.keras.models.load_model('/workspace')\n",
    "\n",
    "# Uncomment what you want to run\n",
    "evaluator = Evaluator(model, loader)\n",
    "#evaluator.evaluate('loss', history)\n",
    "#evaluator.evaluate('token_prob', token_index=10, n_samples=3)\n",
    "evaluator.evaluate('training_sample_pred', n_samples=3)\n",
    "\n",
    "input_text = \"Print 'hello world' followed by 2*n evaluations\"\n",
    "#evaluator.evaluate('manual_sample_pred', input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63cd5725-9961-414f-a052-207233613908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ff1e0a501bd7505c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ff1e0a501bd7505c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8088;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --host localhost --port 8088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa2755c7-0765-457e-8843-b235dc3a30df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#len(loader.problem_tokenizer.word_index) + 1\n",
    "#len(loader.solution_tokenizer.word_index) + 1\n",
    "\n",
    "# Clears logs folder\n",
    "#!find logs -mindepth 1 -delete\n",
    "\n",
    "#print(\"hello\")\n",
    "\n",
    "#print(tf.sysconfig.get_build_info())\n",
    "\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
