{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1c8447c-afb5-46b5-9b44-023332d5d296",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Enable full debugging\\n#tf.config.optimizer.set_jit(False)  # Disable XLA compilation\\ntf.config.run_functions_eagerly(True)\\ntf.data.experimental.enable_debug_mode() # Disables tf.data eager execution; not covered by run_functions_eagerly\\n\\ntf.debugging.experimental.enable_dump_debug_info(\\n    debug_log_dir,\\n    tensor_debug_mode=\"NO_TENSOR\", # CONCISE_HEALTH\\n    circular_buffer_size=1000) # 1000\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Block 1\n",
    "# Imports\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tokenize\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorboard\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "base_log_dir = os.path.join(\"/workspace/logs\", \"run_\" + datetime.datetime.now().strftime(\"%m_%d_%H_%M\"))\n",
    "fit_log_dir = os.path.join(base_log_dir, \"fit\")\n",
    "loader_log_dir = os.path.join(base_log_dir, \"loader\")\n",
    "debug_log_dir = os.path.join(base_log_dir, \"debug\")\n",
    "\n",
    "\"\"\"\n",
    "# Enable full debugging\n",
    "#tf.config.optimizer.set_jit(False)  # Disable XLA compilation\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode() # Disables tf.data eager execution; not covered by run_functions_eagerly\n",
    "\n",
    "tf.debugging.experimental.enable_dump_debug_info(\n",
    "    debug_log_dir,\n",
    "    tensor_debug_mode=\"NO_TENSOR\", # CONCISE_HEALTH\n",
    "    circular_buffer_size=1000) # 1000\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd3532f1-59c9-4421-83cb-49c9dd54bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2\n",
    "# Loader\n",
    "\n",
    "\n",
    "class Loader:\n",
    "    def __init__(self, problems_path, submissions_dir, loader_log_dir, max_length_input, max_length_output, batch_size):\n",
    "        self.problems_path = problems_path\n",
    "        self.submissions_dir = submissions_dir\n",
    "        self.max_length_input = max_length_input\n",
    "        self.max_length_output = max_length_output\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.problem_tokenizer = Tokenizer(filters='')\n",
    "        self.solution_tokenizer = Tokenizer(filters='', oov_token='UNK')\n",
    "        self.dataset = None\n",
    "        \n",
    "        self.writer = tf.summary.create_file_writer(loader_log_dir)\n",
    "\n",
    "    def _load_problems_and_solutions(self):\n",
    "        # Load problems\n",
    "        with open(self.problems_path, 'r') as problems_file:\n",
    "            problems_list = json.load(problems_file)\n",
    "        raw_problems = {}\n",
    "        \n",
    "        for problem in problems_list:\n",
    "            problem_id = problem['problem_id']\n",
    "            concatenated_problem = \"XXSTATEMENT {} XXINPUT {} XXOUTPUT {} XXNOTES {} XXEXAMPLES {}\".format(\n",
    "                problem.get('problem_statement', ''),\n",
    "                problem.get('problem_input', ''),\n",
    "                problem.get('problem_output', ''),\n",
    "                problem.get('problem_notes', ''),\n",
    "                problem.get('examples', '')\n",
    "            )\n",
    "            raw_problems[problem_id] = concatenated_problem\n",
    "            \n",
    "        # Load solutions\n",
    "        raw_solutions = [[] for _ in range(len(raw_problems) * 2)] # Estimate allows us to delete up to half\n",
    "        submissions = glob.glob(os.path.join(self.submissions_dir, \"*.py\"))\n",
    "        \n",
    "        for submission_path in submissions:\n",
    "            problem_number = int(re.findall(r'^\\d+', os.path.basename(submission_path))[0])\n",
    "            with open(submission_path, \"r\") as f:\n",
    "                solutionList = []\n",
    "                for token in tokenize.generate_tokens(f.readline):\n",
    "                    solutionList.append(token.string)\n",
    "                raw_solutions[problem_number].append(solutionList)\n",
    "        \n",
    "        # Combine problems and solutions\n",
    "        problems = []\n",
    "        solutions = []\n",
    "        for problem_id, solution_set in enumerate(raw_solutions):\n",
    "            if solution_set:\n",
    "                for solution in solution_set:\n",
    "                    problems.append(raw_problems[problem_id])\n",
    "                    solutions.append(solution)\n",
    "\n",
    "        return problems, solutions\n",
    "\n",
    "    def tokenize_and_pad(self, problems, solutions):\n",
    "        # Add SOS and EOS tokens\n",
    "        decoder_inputs = [[\"XXSOS\"] + s for s in solutions]\n",
    "        targets = [s + [\"XXEOS\"] for s in solutions]\n",
    "        \n",
    "        # Fit Keras tokenizer\n",
    "        self.problem_tokenizer.fit_on_texts(problems)\n",
    "        self.solution_tokenizer.fit_on_texts(decoder_inputs + targets) # Both at once\n",
    "        \n",
    "        # Tokenize\n",
    "        problems = self.problem_tokenizer.texts_to_sequences(problems)\n",
    "        decoder_inputs = self.solution_tokenizer.texts_to_sequences(decoder_inputs)\n",
    "        targets = self.solution_tokenizer.texts_to_sequences(targets)\n",
    "        \n",
    "        # Pad to same length\n",
    "        problems = pad_sequences(problems, padding='post', maxlen=self.max_length_input)\n",
    "        decoder_inputs = pad_sequences(decoder_inputs, padding='post', maxlen=self.max_length_output)\n",
    "        targets = pad_sequences(targets, padding='post', maxlen=self.max_length_output)\n",
    "        \n",
    "        return problems, decoder_inputs, targets\n",
    "    \n",
    "    def log_vocabulary(self):\n",
    "        with self.writer.as_default():\n",
    "            for word, index in self.problem_tokenizer.word_index.items():\n",
    "                tf.summary.text(name=\"Problem Vocabulary\", data=f\"{word}: {index}\", step=0)\n",
    "\n",
    "            for word, index in self.solution_tokenizer.word_index.items():\n",
    "                tf.summary.text(name=\"Solution Vocabulary\", data=f\"{word}: {index}\", step=0)\n",
    "\n",
    "            self.writer.flush()\n",
    "    \n",
    "    def _create_tf_dataset(self, problems, decoder_inputs, targets):\n",
    "        # Create the dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(((problems, decoder_inputs), targets))\n",
    "        \n",
    "        return dataset.shuffle(buffer_size=1024).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    def _log_dataset_samples(self, problems, decoder_inputs, targets):\n",
    "        with self.writer.as_default():\n",
    "            for i, (problem, decoder_input, target) in enumerate(zip(problems, decoder_inputs, targets)):\n",
    "                if i >= 5: # Log 5 samples\n",
    "                    break\n",
    "                \n",
    "                # Convert padded sequences back to text\n",
    "                problem_text = self.problem_tokenizer.sequences_to_texts([problem])\n",
    "                decoder_input_text = self.solution_tokenizer.sequences_to_texts([decoder_input])\n",
    "                target_text = self.solution_tokenizer.sequences_to_texts([target])\n",
    "\n",
    "                # Truncate texts\n",
    "                #max_display_length = 1024\n",
    "                #problem_text = (problem_text[:max_display_length] + '...') if len(problem_text) > max_display_length else problem_text\n",
    "                #solution_text = (solution_text[:max_display_length] + '...') if len(solution_text) > max_display_length else solution_text\n",
    "\n",
    "                # Log to TensorBoard\n",
    "                tf.summary.text(name=f\"problem_{i}\", data=problem_text, step=0)\n",
    "                tf.summary.text(name=f\"decoder_input_{i}\", data=decoder_input_text, step=0)\n",
    "                tf.summary.text(name=f\"target_{i}\", data=target_text, step=0)\n",
    "\n",
    "            self.writer.flush()\n",
    "            \n",
    "    def load_data(self):\n",
    "        problems, solutions = self._load_problems_and_solutions()\n",
    "\n",
    "        problems, decoder_inputs, targets = self.tokenize_and_pad(problems, solutions)\n",
    "        self._log_dataset_samples(problems, decoder_inputs, targets)\n",
    "        self.dataset = self._create_tf_dataset(problems, decoder_inputs, targets)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd0ef91d-0806-4046-a111-f1916d98f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3\n",
    "# Positional Encoder\n",
    "\n",
    "def positional_encoder(seq_length, dim):\n",
    "    # Generate positions for each element\n",
    "    positions = tf.range(seq_length, dtype=tf.float32)[..., tf.newaxis]\n",
    "\n",
    "    # Create a range for the dimensions and compute division terms\n",
    "    i = tf.range(dim, dtype=tf.float32)\n",
    "    div_terms = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(dim, tf.float32))\n",
    "\n",
    "    # Calculate odd/even sinusoidal encodings\n",
    "    angle_rates = positions * div_terms\n",
    "    sine = tf.sin(angle_rates[:, 0::2])\n",
    "    cosine = tf.cos(angle_rates[:, 1::2])\n",
    "\n",
    "    # Interlace and reshape\n",
    "    pos_encoding = tf.reshape(tf.concat([sine, cosine], axis=-1), [1, seq_length, dim])\n",
    "\n",
    "    return pos_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d505a2d-a02c-4bc5-9ca5-7555f9418fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4\n",
    "# Encoder/Decoder Layer classes\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, dim_ff, key_dim, num_heads, dropout_rate, name=\"EncoderLayer\"):\n",
    "        super(EncoderLayer, self).__init__(name=name)\n",
    "\n",
    "        # Multi-Head Self-Attention layer\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "\n",
    "        # Feed-Forward Network Layers\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim_ff, kernel_initializer='he_normal', name=\"encoder_ffn_dense1\"), \n",
    "            tf.keras.layers.LeakyReLU(alpha=0.01), # Trying LeakyReLu\n",
    "            tf.keras.layers.Dense(dim, kernel_initializer='he_normal', name=\"encoder_ffn_dense2\")\n",
    "        ], name=\"encoder_ffn\")\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"encoder_layernorm1\")\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"encoder_layernorm2\")\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_mha = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # Self-Attention\n",
    "        attn_output = self.mha(x, x)\n",
    "        attn_output = self.dropout_mha(attn_output, training=training) # Dropout\n",
    "        out1 = self.layernorm1(x + attn_output)  # Residual connection\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training) # Dropout\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection\n",
    "\n",
    "        return out2\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(EncoderLayer, self).get_config()\n",
    "        mha_config = self.mha.get_config()\n",
    "        config.update({\n",
    "            \"dim\": self.ffn.layers[2].units, \n",
    "            \"dim_ff\": self.ffn.layers[0].units, \n",
    "            \"num_heads\": mha_config['num_heads'], \n",
    "            \"key_dim\": mha_config['key_dim'], \n",
    "            \"dropout_rate\": self.dropout_mha.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, dim_ff, key_dim, num_heads, dropout_rate, name=\"DecoderLayer\"):\n",
    "        super(DecoderLayer, self).__init__(name=name)\n",
    "\n",
    "        # Self-Attention and Cross-Attention layers\n",
    "        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "\n",
    "        # Feed Forward Network Layers\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim_ff, kernel_initializer='he_normal', name=\"decoder_ffn_dense1\"), \n",
    "            tf.keras.layers.LeakyReLU(alpha=0.01), # Trying LeakyReLu\n",
    "            tf.keras.layers.Dense(dim, kernel_initializer='he_normal', name=\"decoder_ffn_dense2\")\n",
    "        ], name=\"decoder_ffn\")\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm1\")\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm2\")\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm3\")\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_self_attn = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout_cross_attn = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        # Self-Attention\n",
    "        attn1_output = self.mha1(x, x, attention_mask=look_ahead_mask)\n",
    "        attn1_output = self.dropout_self_attn(attn1_output, training=training) # Dropout\n",
    "        out1 = self.layernorm1(x + attn1_output)  # Residual connection\n",
    "\n",
    "        # Cross-Attention\n",
    "        attn2_output = self.mha2(out1, enc_output, attention_mask=padding_mask)\n",
    "        attn2_output = self.dropout_cross_attn(attn2_output, training=training) # Dropout\n",
    "        out2 = self.layernorm2(out1 + attn2_output)  # Residual connection\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training) # Dropout\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # Residual connection\n",
    "\n",
    "        return out3\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DecoderLayer, self).get_config()\n",
    "        mha_config = self.mha1.get_config()  # Assumes mha1 and mha2 have the same configuration\n",
    "        config.update({\n",
    "            'dim': self.ffn.layers[2].units,\n",
    "            'dim_ff': self.ffn.layers[0].units,\n",
    "            'num_heads': mha_config['num_heads'],\n",
    "            'key_dim': mha_config['key_dim'],\n",
    "            'dropout_rate': self.dropout_self_attn.rate\n",
    "        })\n",
    "        return config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bcb9091-5ff0-4c54-89c8-ee0a5463416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5\n",
    "# Transformer\n",
    "\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"TransformerEncoder\"):\n",
    "        super(TransformerEncoder, self).__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [EncoderLayer(dim, dim_ff, key_dim, num_heads, dropout_rate, name=f\"encoder_layer_{i}\") for i in range(num_layers)]\n",
    "\n",
    "    def call(self, x: tf.Tensor, training=False) -> tf.Tensor:\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"TransformerDecoder\"):\n",
    "        super(TransformerDecoder, self).__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.dec_layers = [DecoderLayer(dim, dim_ff, key_dim, num_heads, dropout_rate, name=f\"decoder_layer_{i}\") for i in range(num_layers)]\n",
    "\n",
    "    def call(self, x: tf.Tensor, enc_output: tf.Tensor, training=False) -> tf.Tensor:\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, enc_output, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, dim, dim_ff, key_dim, problem_vocab_size, solution_vocab_size, num_heads, num_layers, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Separate embedding for input and output\n",
    "        self.problem_embedding_layer = tf.keras.layers.Embedding(problem_vocab_size, dim, mask_zero=True)\n",
    "        self.solution_embedding_layer = tf.keras.layers.Embedding(solution_vocab_size, dim, mask_zero=True)\n",
    "\n",
    "        self.encoder = TransformerEncoder(dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"encoder\")\n",
    "        self.decoder = TransformerDecoder(dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"decoder\")\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(solution_vocab_size, name=\"output_layer\")\n",
    "\n",
    "    def call(self, encoder_input, decoder_input, training=False):\n",
    "        encoder_emb = self.problem_embedding_layer(encoder_input)\n",
    "        decoder_emb = self.solution_embedding_layer(decoder_input)\n",
    "\n",
    "        seq_length_enc = tf.shape(encoder_input)[1]\n",
    "        seq_length_dec = tf.shape(decoder_input)[1]\n",
    "        pos_encoding_enc = positional_encoder(seq_length_enc, self.dim)\n",
    "        pos_encoding_dec = positional_encoder(seq_length_dec, self.dim)\n",
    "\n",
    "        encoder_emb += pos_encoding_enc\n",
    "        decoder_emb += pos_encoding_dec\n",
    "\n",
    "        encoder_output = self.encoder(encoder_emb, training=training)\n",
    "        decoder_output = self.decoder(decoder_emb, encoder_output, training=training)\n",
    "\n",
    "        final_output = self.final_layer(decoder_output)\n",
    "\n",
    "        return final_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc04145d-725e-4db0-a9a6-ab2682e60890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6\n",
    "# Build and Compile\n",
    "\n",
    "def build_and_compile(dim, dim_ff, key_dim, nhead, num_layers, problem_vocab_size, solution_vocab_size, dropout_rate, learning_rate=1e-4):\n",
    "    # Define model inputs\n",
    "    encoder_input = tf.keras.Input(shape=(None,), dtype='int32', name='encoder_input')\n",
    "    decoder_input = tf.keras.Input(shape=(None,), dtype='int32', name='decoder_input')\n",
    "\n",
    "    # Initialize and call the Transformer\n",
    "    transformer = Transformer(dim, dim_ff, key_dim, problem_vocab_size, solution_vocab_size, nhead, num_layers, dropout_rate)\n",
    "    final_output = transformer(encoder_input, decoder_input)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=final_output)\n",
    "\n",
    "    # Compile the model\n",
    "    #lr_schedule = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.005, decay_steps=500, alpha=0.0001)\n",
    "    # dataset size / batch size times epochs, is time until decay to alpha\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(ignore_class=0, from_logits=True),\n",
    "        metrics=['accuracy'],\n",
    "        run_eagerly=False # !CAUTION!\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c79e4070-2bbe-423d-8228-4e9aa08347d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7\n",
    "# Define Training Steps\n",
    "\n",
    "def calculate_loss(model_output, tokenized_code, mask):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(tokenized_code, model_output, from_logits=True)\n",
    "    loss *= mask  # Apply mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, tokenized_question, tokenized_code, clip_norm=10.0):\n",
    "    with tf.GradientTape() as tape:\n",
    "        model_output = model([tokenized_question, tokenized_code], training=True)\n",
    "\n",
    "        # Mask PAD tokens\n",
    "        mask = tf.cast(tf.math.logical_not(tf.math.equal(tokenized_code, 0)), dtype=model_output.dtype)\n",
    "        \n",
    "        # Calculate loss\n",
    "        average_loss = calculate_loss(model_output, tokenized_code, mask)\n",
    "\n",
    "    # Compute and clip gradients\n",
    "    gradients = tape.gradient(average_loss, model.trainable_variables)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm)\n",
    "\n",
    "    # Apply gradients to update model weights\n",
    "    optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n",
    "\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d03c601c-7456-48fe-8bbc-bbef9a8a469d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "17/17 [==============================] - 26s 280ms/step - loss: 5.5849 - accuracy: 0.1721\n",
      "Epoch 2/125\n",
      "17/17 [==============================] - 4s 251ms/step - loss: 4.7315 - accuracy: 0.2071\n",
      "Epoch 3/125\n",
      "17/17 [==============================] - 4s 237ms/step - loss: 4.3272 - accuracy: 0.2192\n",
      "Epoch 4/125\n",
      "17/17 [==============================] - 4s 223ms/step - loss: 4.0526 - accuracy: 0.2375\n",
      "Epoch 5/125\n",
      "17/17 [==============================] - 4s 225ms/step - loss: 3.8755 - accuracy: 0.2426\n",
      "Epoch 6/125\n",
      "17/17 [==============================] - 4s 241ms/step - loss: 3.7640 - accuracy: 0.2485\n",
      "Epoch 7/125\n",
      "17/17 [==============================] - 4s 213ms/step - loss: 3.6689 - accuracy: 0.2614\n",
      "Epoch 8/125\n",
      "17/17 [==============================] - 4s 224ms/step - loss: 3.5285 - accuracy: 0.2960\n",
      "Epoch 9/125\n",
      "17/17 [==============================] - 4s 221ms/step - loss: 3.4224 - accuracy: 0.3153\n",
      "Epoch 10/125\n",
      "17/17 [==============================] - 4s 217ms/step - loss: 3.2941 - accuracy: 0.3405\n",
      "Epoch 11/125\n",
      "17/17 [==============================] - 4s 216ms/step - loss: 3.2006 - accuracy: 0.3506\n",
      "Epoch 12/125\n",
      "17/17 [==============================] - 4s 234ms/step - loss: 3.1055 - accuracy: 0.3660\n",
      "Epoch 13/125\n",
      "17/17 [==============================] - 4s 214ms/step - loss: 3.0179 - accuracy: 0.3792\n",
      "Epoch 14/125\n",
      "17/17 [==============================] - 4s 217ms/step - loss: 2.9480 - accuracy: 0.3850\n",
      "Epoch 15/125\n",
      "17/17 [==============================] - 4s 219ms/step - loss: 2.8749 - accuracy: 0.3923\n",
      "Epoch 16/125\n",
      "17/17 [==============================] - 4s 216ms/step - loss: 2.8068 - accuracy: 0.4029\n",
      "Epoch 17/125\n",
      "17/17 [==============================] - 4s 248ms/step - loss: 2.7533 - accuracy: 0.4069\n",
      "Epoch 18/125\n",
      "17/17 [==============================] - 4s 226ms/step - loss: 2.6937 - accuracy: 0.4160\n",
      "Epoch 19/125\n",
      "17/17 [==============================] - 4s 229ms/step - loss: 2.6457 - accuracy: 0.4196\n",
      "Epoch 20/125\n",
      "17/17 [==============================] - 4s 208ms/step - loss: 2.6060 - accuracy: 0.4250\n",
      "Epoch 21/125\n",
      "17/17 [==============================] - 4s 219ms/step - loss: 2.5668 - accuracy: 0.4320\n",
      "Epoch 22/125\n",
      "17/17 [==============================] - 4s 220ms/step - loss: 2.5369 - accuracy: 0.4343\n",
      "Epoch 23/125\n",
      "17/17 [==============================] - 3s 209ms/step - loss: 2.5258 - accuracy: 0.4316\n",
      "Epoch 24/125\n",
      "17/17 [==============================] - 4s 216ms/step - loss: 2.4751 - accuracy: 0.4429\n",
      "Epoch 25/125\n",
      "17/17 [==============================] - 4s 234ms/step - loss: 2.4394 - accuracy: 0.4501\n",
      "Epoch 26/125\n",
      "17/17 [==============================] - 4s 224ms/step - loss: 2.4152 - accuracy: 0.4524\n",
      "Epoch 27/125\n",
      "17/17 [==============================] - 3s 209ms/step - loss: 2.3925 - accuracy: 0.4544\n",
      "Epoch 28/125\n",
      "17/17 [==============================] - 4s 232ms/step - loss: 2.3587 - accuracy: 0.4597\n",
      "Epoch 29/125\n",
      "17/17 [==============================] - 3s 204ms/step - loss: 2.3412 - accuracy: 0.4657\n",
      "Epoch 30/125\n",
      "17/17 [==============================] - 4s 215ms/step - loss: 2.3019 - accuracy: 0.4715\n",
      "Epoch 31/125\n",
      "17/17 [==============================] - 4s 214ms/step - loss: 2.2752 - accuracy: 0.4764\n",
      "Epoch 32/125\n",
      "17/17 [==============================] - 3s 208ms/step - loss: 2.2588 - accuracy: 0.4779\n",
      "Epoch 33/125\n",
      "17/17 [==============================] - 4s 218ms/step - loss: 2.2164 - accuracy: 0.4886\n",
      "Epoch 34/125\n",
      "17/17 [==============================] - 4s 212ms/step - loss: 2.1878 - accuracy: 0.4950\n",
      "Epoch 35/125\n",
      "17/17 [==============================] - 3s 206ms/step - loss: 2.1976 - accuracy: 0.4921\n",
      "Epoch 36/125\n",
      "17/17 [==============================] - 3s 208ms/step - loss: 2.1458 - accuracy: 0.5016\n",
      "Epoch 37/125\n",
      "17/17 [==============================] - 4s 209ms/step - loss: 2.1061 - accuracy: 0.5123\n",
      "Epoch 38/125\n",
      "17/17 [==============================] - 4s 224ms/step - loss: 2.0676 - accuracy: 0.5190\n",
      "Epoch 39/125\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 2.0535 - accuracy: 0.5212\n",
      "Epoch 40/125\n",
      "17/17 [==============================] - 4s 224ms/step - loss: 2.0256 - accuracy: 0.5281\n",
      "Epoch 41/125\n",
      "17/17 [==============================] - 4s 222ms/step - loss: 1.9782 - accuracy: 0.5380\n",
      "Epoch 42/125\n",
      "17/17 [==============================] - 3s 198ms/step - loss: 1.9452 - accuracy: 0.5445\n",
      "Epoch 43/125\n",
      "17/17 [==============================] - 4s 214ms/step - loss: 1.9003 - accuracy: 0.5573\n",
      "Epoch 44/125\n",
      "17/17 [==============================] - 4s 227ms/step - loss: 1.8438 - accuracy: 0.5702\n",
      "Epoch 45/125\n",
      "17/17 [==============================] - 4s 212ms/step - loss: 1.7617 - accuracy: 0.5913\n",
      "Epoch 46/125\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 1.7202 - accuracy: 0.6015\n",
      "Epoch 47/125\n",
      "17/17 [==============================] - 4s 212ms/step - loss: 1.6061 - accuracy: 0.6320\n",
      "Epoch 48/125\n",
      "17/17 [==============================] - 3s 201ms/step - loss: 1.5358 - accuracy: 0.6523\n",
      "Epoch 49/125\n",
      "17/17 [==============================] - 4s 213ms/step - loss: 1.4614 - accuracy: 0.6674\n",
      "Epoch 50/125\n",
      "17/17 [==============================] - 3s 194ms/step - loss: 1.3896 - accuracy: 0.6828\n",
      "Epoch 51/125\n",
      "17/17 [==============================] - 4s 226ms/step - loss: 1.3280 - accuracy: 0.6972\n",
      "Epoch 52/125\n",
      "17/17 [==============================] - 4s 222ms/step - loss: 1.2589 - accuracy: 0.7134\n",
      "Epoch 53/125\n",
      "17/17 [==============================] - 3s 206ms/step - loss: 1.1981 - accuracy: 0.7270\n",
      "Epoch 54/125\n",
      "17/17 [==============================] - 3s 200ms/step - loss: 1.1480 - accuracy: 0.7389\n",
      "Epoch 55/125\n",
      "17/17 [==============================] - 3s 201ms/step - loss: 1.1088 - accuracy: 0.7466\n",
      "Epoch 56/125\n",
      "17/17 [==============================] - 4s 214ms/step - loss: 1.0626 - accuracy: 0.7549\n",
      "Epoch 57/125\n",
      "17/17 [==============================] - 3s 207ms/step - loss: 1.0338 - accuracy: 0.7612\n",
      "Epoch 58/125\n",
      "17/17 [==============================] - 3s 201ms/step - loss: 0.9914 - accuracy: 0.7679\n",
      "Epoch 59/125\n",
      "17/17 [==============================] - 3s 209ms/step - loss: 0.9585 - accuracy: 0.7760\n",
      "Epoch 60/125\n",
      "17/17 [==============================] - 4s 211ms/step - loss: 0.9181 - accuracy: 0.7856\n",
      "Epoch 61/125\n",
      "17/17 [==============================] - 4s 218ms/step - loss: 0.8808 - accuracy: 0.7946\n",
      "Epoch 62/125\n",
      "17/17 [==============================] - 3s 202ms/step - loss: 0.8487 - accuracy: 0.8043\n",
      "Epoch 63/125\n",
      "17/17 [==============================] - 4s 214ms/step - loss: 0.8243 - accuracy: 0.8086\n",
      "Epoch 64/125\n",
      "17/17 [==============================] - 4s 216ms/step - loss: 0.7941 - accuracy: 0.8154\n",
      "Epoch 65/125\n",
      "17/17 [==============================] - 4s 206ms/step - loss: 0.7723 - accuracy: 0.8198\n",
      "Epoch 66/125\n",
      "17/17 [==============================] - 3s 200ms/step - loss: 0.7436 - accuracy: 0.8263\n",
      "Epoch 67/125\n",
      "17/17 [==============================] - 3s 191ms/step - loss: 0.7112 - accuracy: 0.8325\n",
      "Epoch 68/125\n",
      "17/17 [==============================] - 3s 209ms/step - loss: 0.6865 - accuracy: 0.8394\n",
      "Epoch 69/125\n",
      "17/17 [==============================] - 3s 210ms/step - loss: 0.6667 - accuracy: 0.8416\n",
      "Epoch 70/125\n",
      "17/17 [==============================] - 4s 215ms/step - loss: 0.6427 - accuracy: 0.8479\n",
      "Epoch 71/125\n",
      "17/17 [==============================] - 4s 220ms/step - loss: 0.6233 - accuracy: 0.8524\n",
      "Epoch 72/125\n",
      "17/17 [==============================] - 4s 219ms/step - loss: 0.5987 - accuracy: 0.8555\n",
      "Epoch 73/125\n",
      "17/17 [==============================] - 3s 207ms/step - loss: 0.5866 - accuracy: 0.8584\n",
      "Epoch 74/125\n",
      "17/17 [==============================] - 3s 204ms/step - loss: 0.5648 - accuracy: 0.8638\n",
      "Epoch 75/125\n",
      "17/17 [==============================] - 3s 188ms/step - loss: 0.5479 - accuracy: 0.8679\n",
      "Epoch 76/125\n",
      "17/17 [==============================] - 3s 207ms/step - loss: 0.5340 - accuracy: 0.8684\n",
      "Epoch 77/125\n",
      "17/17 [==============================] - 3s 205ms/step - loss: 0.5253 - accuracy: 0.8721\n",
      "Epoch 78/125\n",
      "17/17 [==============================] - 4s 212ms/step - loss: 0.5026 - accuracy: 0.8758\n",
      "Epoch 79/125\n",
      "17/17 [==============================] - 4s 212ms/step - loss: 0.5035 - accuracy: 0.8753\n",
      "Epoch 80/125\n",
      "17/17 [==============================] - 3s 204ms/step - loss: 0.4794 - accuracy: 0.8812\n",
      "Epoch 81/125\n",
      "17/17 [==============================] - 4s 217ms/step - loss: 0.4649 - accuracy: 0.8840\n",
      "Epoch 82/125\n",
      "17/17 [==============================] - 4s 221ms/step - loss: 0.4518 - accuracy: 0.8859\n",
      "Epoch 83/125\n",
      "17/17 [==============================] - 4s 224ms/step - loss: 0.4455 - accuracy: 0.8886\n",
      "Epoch 84/125\n",
      "17/17 [==============================] - 4s 222ms/step - loss: 0.4310 - accuracy: 0.8920\n",
      "Epoch 85/125\n",
      "17/17 [==============================] - 4s 218ms/step - loss: 0.4244 - accuracy: 0.8924\n",
      "Epoch 86/125\n",
      "17/17 [==============================] - 4s 215ms/step - loss: 0.4071 - accuracy: 0.8957\n",
      "Epoch 87/125\n",
      "17/17 [==============================] - 4s 226ms/step - loss: 0.3993 - accuracy: 0.8980\n",
      "Epoch 88/125\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 0.3917 - accuracy: 0.8980\n",
      "Epoch 89/125\n",
      "17/17 [==============================] - 3s 209ms/step - loss: 0.3749 - accuracy: 0.9028\n",
      "Epoch 90/125\n",
      "17/17 [==============================] - 4s 213ms/step - loss: 0.3670 - accuracy: 0.9042\n",
      "Epoch 91/125\n",
      "17/17 [==============================] - 4s 208ms/step - loss: 0.3565 - accuracy: 0.9057\n",
      "Epoch 92/125\n",
      "17/17 [==============================] - 4s 217ms/step - loss: 0.3511 - accuracy: 0.9064\n",
      "Epoch 93/125\n",
      "17/17 [==============================] - 4s 218ms/step - loss: 0.3423 - accuracy: 0.9090\n",
      "Epoch 94/125\n",
      "17/17 [==============================] - 4s 213ms/step - loss: 0.3330 - accuracy: 0.9120\n",
      "Epoch 95/125\n",
      "17/17 [==============================] - 4s 230ms/step - loss: 0.3267 - accuracy: 0.9121\n",
      "Epoch 96/125\n",
      "17/17 [==============================] - 3s 198ms/step - loss: 0.3208 - accuracy: 0.9128\n",
      "Epoch 97/125\n",
      "17/17 [==============================] - 3s 203ms/step - loss: 0.3166 - accuracy: 0.9137\n",
      "Epoch 98/125\n",
      "17/17 [==============================] - 4s 208ms/step - loss: 0.3053 - accuracy: 0.9155\n",
      "Epoch 99/125\n",
      "17/17 [==============================] - 4s 220ms/step - loss: 0.2930 - accuracy: 0.9202\n",
      "Epoch 100/125\n",
      "17/17 [==============================] - 4s 216ms/step - loss: 0.2875 - accuracy: 0.9199\n",
      "Epoch 101/125\n",
      "17/17 [==============================] - 3s 202ms/step - loss: 0.2851 - accuracy: 0.9202\n",
      "Epoch 102/125\n",
      "17/17 [==============================] - 3s 205ms/step - loss: 0.2801 - accuracy: 0.9221\n",
      "Epoch 103/125\n",
      "17/17 [==============================] - 3s 202ms/step - loss: 0.2685 - accuracy: 0.9240\n",
      "Epoch 104/125\n",
      "17/17 [==============================] - 3s 207ms/step - loss: 0.2628 - accuracy: 0.9252\n",
      "Epoch 105/125\n",
      "17/17 [==============================] - 4s 216ms/step - loss: 0.2570 - accuracy: 0.9282\n",
      "Epoch 106/125\n",
      "17/17 [==============================] - 4s 220ms/step - loss: 0.2513 - accuracy: 0.9287\n",
      "Epoch 107/125\n",
      "17/17 [==============================] - 3s 209ms/step - loss: 0.2493 - accuracy: 0.9288\n",
      "Epoch 108/125\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 0.2434 - accuracy: 0.9300\n",
      "Epoch 109/125\n",
      "17/17 [==============================] - 4s 223ms/step - loss: 0.2580 - accuracy: 0.9265\n",
      "Epoch 110/125\n",
      "17/17 [==============================] - 4s 223ms/step - loss: 0.2422 - accuracy: 0.9316\n",
      "Epoch 111/125\n",
      "17/17 [==============================] - 3s 202ms/step - loss: 0.2324 - accuracy: 0.9326\n",
      "Epoch 112/125\n",
      "17/17 [==============================] - 4s 221ms/step - loss: 0.2216 - accuracy: 0.9357\n",
      "Epoch 113/125\n",
      "17/17 [==============================] - 4s 212ms/step - loss: 0.2176 - accuracy: 0.9368\n",
      "Epoch 114/125\n",
      "17/17 [==============================] - 4s 214ms/step - loss: 0.2141 - accuracy: 0.9379\n",
      "Epoch 115/125\n",
      "17/17 [==============================] - 4s 219ms/step - loss: 0.2109 - accuracy: 0.9388\n",
      "Epoch 116/125\n",
      "17/17 [==============================] - 4s 218ms/step - loss: 0.2024 - accuracy: 0.9419\n",
      "Epoch 117/125\n",
      "17/17 [==============================] - 4s 234ms/step - loss: 0.1998 - accuracy: 0.9414\n",
      "Epoch 118/125\n",
      "17/17 [==============================] - 4s 217ms/step - loss: 0.1943 - accuracy: 0.9433\n",
      "Epoch 119/125\n",
      "17/17 [==============================] - 4s 214ms/step - loss: 0.1942 - accuracy: 0.9433\n",
      "Epoch 120/125\n",
      "17/17 [==============================] - 4s 217ms/step - loss: 0.1869 - accuracy: 0.9457\n",
      "Epoch 121/125\n",
      "17/17 [==============================] - 4s 225ms/step - loss: 0.1861 - accuracy: 0.9456\n",
      "Epoch 122/125\n",
      "17/17 [==============================] - 4s 216ms/step - loss: 0.1783 - accuracy: 0.9470\n",
      "Epoch 123/125\n",
      "17/17 [==============================] - 4s 210ms/step - loss: 0.1765 - accuracy: 0.9478\n",
      "Epoch 124/125\n",
      "17/17 [==============================] - 4s 218ms/step - loss: 0.1700 - accuracy: 0.9488\n",
      "Epoch 125/125\n",
      "17/17 [==============================] - 3s 205ms/step - loss: 0.1655 - accuracy: 0.9516\n",
      "INFO:tensorflow:Assets written to: /workspace/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /workspace/assets\n"
     ]
    }
   ],
   "source": [
    "# Block 8\n",
    "# Execution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to data\n",
    "    problems_path = \"/workspace/Training_Data/A_Problems.json\"\n",
    "    submissions_dir = \"/workspace/Training_Data/A_Submissions\"\n",
    "\n",
    "    #problems_path = \"C:/AIClub/Training_Data_Cleaned/A_Problems.json\"\n",
    "    #submissions_dir = \"C:/AIClub/Training_Data_Cleaned/A_Submissions\"\n",
    "    #You'll need to update logging in Block 1\n",
    "\n",
    "    # Set hyperparameters\n",
    "    dim = 256\n",
    "    dim_ff = dim * 4\n",
    "    num_layers = 6\n",
    "    num_heads = 4\n",
    "    key_dim = dim // num_heads\n",
    "\n",
    "    max_length_input = 530 # Set to cover about 85% of inputs\n",
    "    max_length_output = 50 # Set to cover 100% of ground truth outputs\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.0001\n",
    "    epochs = 125\n",
    "\n",
    "    assert dim % num_heads == 0, \"dim % num_heads != 0\"\n",
    "\n",
    "    # Initialize the Loader\n",
    "    loader = Loader(problems_path, submissions_dir, loader_log_dir, max_length_input, max_length_output, batch_size)\n",
    "    loader.load_data()\n",
    "    problem_vocab_size = len(loader.problem_tokenizer.word_index) + 1\n",
    "    solution_vocab_size = len(loader.solution_tokenizer.word_index) + 1\n",
    "    \n",
    "    # Build the model\n",
    "    model = build_and_compile(dim, dim_ff, key_dim, num_heads, num_layers, problem_vocab_size, solution_vocab_size, dropout_rate, learning_rate)\n",
    "\n",
    "    # Setup TensorBoard callback\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=fit_log_dir, histogram_freq=1)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(loader.dataset, epochs=epochs, callbacks=[tensorboard_callback]) # history variable unused...\n",
    "    \n",
    "    \"\"\"\n",
    "    Manual training setup\n",
    "    \n",
    "    optimizer = model.optimizer\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Start of Epoch {epoch+1}\")\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, ((tokenized_question, tokenized_code), target) in enumerate(loader.dataset):\n",
    "            # Call the custom train_step\n",
    "            loss = train_step(model, optimizer, tokenized_question, tokenized_code[:, :-1])\n",
    "\n",
    "            # Log every 200 batches\n",
    "            if step % 200 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        print(f\"End of Epoch {epoch+1}, Loss: {loss.numpy()}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(\"/workspace\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "112669f0-144f-4c82-ae0d-9d5f93ee4fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 525ms/step\n",
      "Sample 1:\n",
      "Input sequence [:250]: xxstatement what do we need caps lock for? caps lock is a computer keyboard key. pressing it sets an input mode in which typed letters are capital by default. if it is pressed by accident, it leads to accidents like the one we had in the first passag\n",
      "Predicted sequence: [ 17 268   3   4   4   2   2   5  37   7   2   2 109  25   6  48  30  25\n",
      "  10  18   7  11   2   2  15   3  25   4   2   2   5   5   5  37   7   2\n",
      "   2  23  15   3  25   4   2   2   2   2   5   5   5  41]\n",
      "Predicted text: . swapcase ( ) ) \n",
      " \n",
      "  else : \n",
      " \n",
      "                  x = y + x [ 0 : ] \n",
      " \n",
      " print ( x ) \n",
      " \n",
      "    else : \n",
      " \n",
      "          print ( x ) \n",
      " \n",
      " \n",
      " \n",
      "    xxeos \n",
      "\n",
      "Sample 2:\n",
      "Input sequence [:250]: xxstatement dima and inna are doing so great! at the moment, inna is sitting on the magic lawn playing with a pink pony. dima wanted to play too. he brought an n  ×  m chessboard, a very tasty candy and two numbers a and b . dima put the chessboard i\n",
      "Predicted sequence: [  2   5 321   6  68   3 321   8  65   3 506   8 507   4   4   2   2   2\n",
      "   2   5  22 122  27 123  17 505   7   2   2  19  15   3 932   4   2   2\n",
      "   5  37   7   2   2  19  15   3 207   4   5   5   5  41]\n",
      "Predicted text: \n",
      "  minsteps = min ( minsteps , max ( maxstepsa , maxstepsb ) ) \n",
      " \n",
      " \n",
      " \n",
      "  if q == math . inf : \n",
      " \n",
      "      print ( \"poor inna and pony!\" ) \n",
      " \n",
      "  else : \n",
      " \n",
      "      print ( \"friendship\" )    xxeos \n",
      "\n",
      "Sample 3:\n",
      "Input sequence [:250]: xxstatement little petya very much likes gifts. recently he has received a new laptop as a new year gift from his mother. he immediately decided to give it to somebody else as what can be more pleasant than giving somebody gifts. and on this occasion\n",
      "Predicted sequence: [14  6 16  3 12  3  4  4 85 28  6 12  3  4 17 31  3  4 85 15  3 35 10 28\n",
      " 17 88  3 67  3 13 30  9  4  4 30  9 24 13 21 32  3 14  4 11  4  5  5 41\n",
      " 41 41]\n",
      "Predicted text: n = int ( input ( ) ) ; s = input ( ) . split ( ) ; print ( * [ s . index ( str ( i + 1 ) ) + 1 for i in range ( n ) ] )   xxeos xxeos xxeos \n",
      "\n",
      "1/1 [==============================] - 1s 531ms/step\n",
      "Input text: XXSTATEMENT Susie like squares.  When playing with one, she wants to know how many sides it has.  For each side of the square, print its length.  XXINPUT An object s, which is an integer representing the length of a side l.  XXOUTPUT Four copies of the length s\n",
      "Predicted text: +=\n"
     ]
    }
   ],
   "source": [
    "# Block 10\n",
    "# Evaluation Class\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, loader):\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "\n",
    "    def plot_loss(self, history):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.title('Loss Curve')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_token_probabilities(self, token_index, n_samples=1):\n",
    "        # Take one batch from the dataset\n",
    "        for (encoder_input, decoder_input), _ in self.loader.dataset.take(1):\n",
    "            # Slice the batch down to n_samples\n",
    "            encoder_input = encoder_input[:n_samples]\n",
    "            decoder_input = decoder_input[:n_samples]\n",
    "\n",
    "            # Predict on the sliced inputs\n",
    "            predictions = self.model.predict([encoder_input, decoder_input])\n",
    "\n",
    "            for sample_idx in range(n_samples):\n",
    "                # For each sample, extract token logits and convert to probabilities\n",
    "                token_logits = predictions[sample_idx, token_index, :]\n",
    "                token_probabilities = tf.nn.softmax(token_logits).numpy()\n",
    "                sorted_indices = np.argsort(token_probabilities)[::-1]\n",
    "                sorted_probabilities = token_probabilities[sorted_indices]\n",
    "\n",
    "                # Plotting\n",
    "                plt.figure(figsize=(20, 5))\n",
    "                plt.bar(range(len(sorted_probabilities)), sorted_probabilities)\n",
    "                plt.xlabel('Word Indices (sorted by probability)')\n",
    "                plt.ylabel('Probability')\n",
    "                plt.title(f'Word Prediction Probabilities for Token {token_index} in Sample {sample_idx+1}')\n",
    "                plt.show()\n",
    "\n",
    "    def generate_training_predictions(self, n_samples=1):\n",
    "        # Take one batch from the dataset\n",
    "        for (encoder_inputs, decoder_inputs), _ in self.loader.dataset.take(1):\n",
    "            # Slice the batch down to n_samples\n",
    "            encoder_inputs = encoder_inputs[:n_samples]\n",
    "            decoder_inputs = decoder_inputs[:n_samples]\n",
    "\n",
    "            # Predict on the sliced inputs\n",
    "            predictions = self.model.predict([encoder_inputs, decoder_inputs])\n",
    "            predicted_sequences = np.argmax(predictions, axis=-1)\n",
    "\n",
    "            # Convert sequences to text\n",
    "            encoder_inputs = encoder_inputs = encoder_inputs.numpy() # Can factor this later\n",
    "            input_texts = self.loader.problem_tokenizer.sequences_to_texts(encoder_inputs)\n",
    "            predicted_texts = self.loader.solution_tokenizer.sequences_to_texts(predicted_sequences)\n",
    "            \n",
    "            # Print each prediction in the slice\n",
    "            for i, predicted_text in enumerate(predicted_texts):\n",
    "                print(f\"Sample {i + 1}:\") \n",
    "                print(\"Input sequence [:250]:\", input_texts[i][:250])\n",
    "                print(\"Predicted sequence:\", predicted_sequences[i])\n",
    "                print(\"Predicted text:\", predicted_text, \"\\n\")\n",
    "    \n",
    "    def generate_manual_predictions(self, input_text):\n",
    "        # Tokenize the input string\n",
    "        input_seq = self.loader.problem_tokenizer.texts_to_sequences([input_text])\n",
    "        input_padded = tf.keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=self.loader.max_length_input, padding='post')\n",
    "\n",
    "        # Prepare the decoder input\n",
    "        start_token_index = self.loader.solution_tokenizer.word_index.get('[START]', 1)  # Fallback to 1 if not found\n",
    "        decoder_input = np.array([[start_token_index]])\n",
    "        \n",
    "        # Generate and interpret the prediction\n",
    "        predictions = self.model.predict([input_padded, decoder_input])\n",
    "        predicted_sequence = np.argmax(predictions, axis=-1)[0]\n",
    "        predicted_text = self.loader.solution_tokenizer.sequences_to_texts([predicted_sequence])\n",
    "        \n",
    "        # Print the output\n",
    "        print(\"Input text:\", input_text)\n",
    "        print(\"Predicted text:\", predicted_text[0])\n",
    "\n",
    "    def evaluate(self, command, *args, **kwargs):\n",
    "        if command == 'loss':\n",
    "            self.plot_loss(*args, **kwargs)\n",
    "        elif command == 'token_prob':\n",
    "            self.plot_token_probabilities(*args, **kwargs)\n",
    "        elif command == 'training_sample_pred':\n",
    "            self.generate_training_predictions(*args, **kwargs)\n",
    "        elif command == 'manual_sample_pred':\n",
    "            self.generate_manual_predictions(*args, **kwargs)\n",
    "        else:\n",
    "            print(f\"Unknown command: {command}\")\n",
    "\n",
    "# Load the model if it's not\n",
    "model = tf.keras.models.load_model('/workspace')\n",
    "\n",
    "# Uncomment what you want to run\n",
    "evaluator = Evaluator(model, loader)\n",
    "#evaluator.evaluate('loss', history)\n",
    "#evaluator.evaluate('token_prob', token_index=10, n_samples=3)\n",
    "evaluator.evaluate('training_sample_pred', n_samples=3)\n",
    "\n",
    "input_text = \"XXSTATEMENT Susie like squares.  When playing with one, she wants to know how many sides it has.  For each side of the square, print its length.  XXINPUT An object s, which is an integer representing the length of a side l.  XXOUTPUT Four copies of the length s\"\n",
    "evaluator.evaluate('manual_sample_pred', input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63cd5725-9961-414f-a052-207233613908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 8088 (pid 13098), started 0:04:24 ago. (Use '!kill 13098' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a49890edd2db5126\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a49890edd2db5126\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8088;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --host localhost --port 8088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa2755c7-0765-457e-8843-b235dc3a30df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(len(loader.problem_tokenizer.word_index) + 1)\n",
    "#print(len(loader.solution_tokenizer.word_index) + 1)\n",
    "\n",
    "# Clears logs folder\n",
    "#!find logs -mindepth 1 -delete\n",
    "\n",
    "#tf.debugging.experimental.disable_dump_debug_info()\n",
    "\n",
    "#!kill 415\n",
    "\n",
    "#print(\"hello\")\n",
    "\n",
    "#print(tf.sysconfig.get_build_info())\n",
    "\n",
    "#tf.__version__\n",
    "#!tensorboard --version\n",
    "\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade tensorboard\n",
    "\n",
    "#f.config.run_functions_eagerly(False)\n",
    "#f.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca6dbb14-f316-47e2-a8a3-16faaf790ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196b8c1-fa3c-422e-be19-bea76c8681f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
