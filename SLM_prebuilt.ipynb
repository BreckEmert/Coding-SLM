{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1c8447c-afb5-46b5-9b44-023332d5d296",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Enable full debugging\\n#tf.config.optimizer.set_jit(False)  # Disable XLA compilation\\ntf.config.run_functions_eagerly(True)\\ntf.data.experimental.enable_debug_mode() # Disables tf.data eager execution; not covered by run_functions_eagerly\\n\\ntf.debugging.experimental.enable_dump_debug_info(\\n    debug_log_dir,\\n    tensor_debug_mode=\"NO_TENSOR\", # CONCISE_HEALTH\\n    circular_buffer_size=1000) # 1000\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Block 1\n",
    "# Imports\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tokenize\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorboard\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "base_log_dir = os.path.join(\"/workspace/logs\", \"run_\" + datetime.datetime.now().strftime(\"%m_%d_%H_%M\"))\n",
    "fit_log_dir = os.path.join(base_log_dir, \"fit\")\n",
    "loader_log_dir = os.path.join(base_log_dir, \"loader\")\n",
    "debug_log_dir = os.path.join(base_log_dir, \"debug\")\n",
    "\n",
    "\"\"\"\n",
    "# Enable full debugging\n",
    "#tf.config.optimizer.set_jit(False)  # Disable XLA compilation\n",
    "tf.config.run_functions_eagerly(True)\n",
    "tf.data.experimental.enable_debug_mode() # Disables tf.data eager execution; not covered by run_functions_eagerly\n",
    "\n",
    "tf.debugging.experimental.enable_dump_debug_info(\n",
    "    debug_log_dir,\n",
    "    tensor_debug_mode=\"NO_TENSOR\", # CONCISE_HEALTH\n",
    "    circular_buffer_size=1000) # 1000\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd3532f1-59c9-4421-83cb-49c9dd54bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2\n",
    "# Loader\n",
    "\n",
    "\n",
    "class Loader:\n",
    "    def __init__(self, problems_path, submissions_dir, loader_log_dir, max_length_input, max_length_output, batch_size):\n",
    "        self.problems_path = problems_path\n",
    "        self.submissions_dir = submissions_dir\n",
    "        self.max_length_input = max_length_input\n",
    "        self.max_length_output = max_length_output\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.problem_tokenizer = Tokenizer(filters='')\n",
    "        self.solution_tokenizer = Tokenizer(filters='', oov_token='UNK')\n",
    "        self.dataset = None\n",
    "        \n",
    "        self.writer = tf.summary.create_file_writer(loader_log_dir)\n",
    "\n",
    "    def _load_problems_and_solutions(self):\n",
    "        # Load problems\n",
    "        with open(self.problems_path, 'r') as problems_file:\n",
    "            problems_list = json.load(problems_file)\n",
    "        raw_problems = {}\n",
    "        \n",
    "        for problem in problems_list:\n",
    "            problem_id = problem['problem_id']\n",
    "            concatenated_problem = \"XXSTATEMENT {} XXINPUT {} XXOUTPUT {} XXNOTES {} XXEXAMPLES {}\".format(\n",
    "                problem.get('problem_statement', ''),\n",
    "                problem.get('problem_input', ''),\n",
    "                problem.get('problem_output', ''),\n",
    "                problem.get('problem_notes', ''),\n",
    "                problem.get('examples', '')\n",
    "            )\n",
    "            raw_problems[problem_id] = concatenated_problem\n",
    "            \n",
    "        # Load solutions\n",
    "        raw_solutions = [[] for _ in range(len(raw_problems) * 2)] # Estimate allows us to delete up to half\n",
    "        submissions = glob.glob(os.path.join(self.submissions_dir, \"*.py\"))\n",
    "        \n",
    "        for submission_path in submissions:\n",
    "            problem_number = int(re.findall(r'^\\d+', os.path.basename(submission_path))[0])\n",
    "            with open(submission_path, \"r\") as f:\n",
    "                solutionList = []\n",
    "                for token in tokenize.generate_tokens(f.readline):\n",
    "                    solutionList.append(token.string)\n",
    "                raw_solutions[problem_number].append(solutionList)\n",
    "        \n",
    "        # Combine problems and solutions\n",
    "        problems = []\n",
    "        solutions = []\n",
    "        for problem_id, solution_set in enumerate(raw_solutions):\n",
    "            if solution_set:\n",
    "                for solution in solution_set:\n",
    "                    problems.append(raw_problems[problem_id])\n",
    "                    solutions.append(solution)\n",
    "\n",
    "        return problems, solutions\n",
    "\n",
    "    def tokenize_and_pad(self, problems, solutions):\n",
    "        # Add SOS and EOS tokens\n",
    "        decoder_inputs = [[\"XXSOS\"] + s for s in solutions]\n",
    "        targets = [s + [\"XXEOS\"] for s in solutions]\n",
    "        \n",
    "        # Fit Keras tokenizer\n",
    "        self.problem_tokenizer.fit_on_texts(problems)\n",
    "        self.solution_tokenizer.fit_on_texts(decoder_inputs + targets) # Both at once\n",
    "        \n",
    "        # Vectorize\n",
    "        problems = self.problem_tokenizer.texts_to_sequences(problems)\n",
    "        decoder_inputs = self.solution_tokenizer.texts_to_sequences(decoder_inputs)\n",
    "        targets = self.solution_tokenizer.texts_to_sequences(targets)\n",
    "        \n",
    "        # Pad to same length\n",
    "        problems = pad_sequences(problems, padding='post', maxlen=self.max_length_input)\n",
    "        decoder_inputs = pad_sequences(decoder_inputs, padding='post', maxlen=self.max_length_output)\n",
    "        targets = pad_sequences(targets, padding='post', maxlen=self.max_length_output)\n",
    "        \n",
    "        return problems, decoder_inputs, targets\n",
    "    \n",
    "    def log_vocabulary(self):\n",
    "        with self.writer.as_default():\n",
    "            for word, index in self.problem_tokenizer.word_index.items():\n",
    "                tf.summary.text(name=\"Problem Vocabulary\", data=f\"{word}: {index}\", step=0)\n",
    "\n",
    "            for word, index in self.solution_tokenizer.word_index.items():\n",
    "                tf.summary.text(name=\"Solution Vocabulary\", data=f\"{word}: {index}\", step=0)\n",
    "\n",
    "            self.writer.flush()\n",
    "    \n",
    "    def _create_tf_dataset(self, problems, decoder_inputs, targets):\n",
    "        # Create the dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(((problems, decoder_inputs), targets))\n",
    "        \n",
    "        return dataset.shuffle(buffer_size=1024).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    def _log_dataset_samples(self, problems, decoder_inputs, targets):\n",
    "        with self.writer.as_default():\n",
    "            for i, (problem, decoder_input, target) in enumerate(zip(problems, decoder_inputs, targets)):\n",
    "                if i >= 5: # Log 5 samples\n",
    "                    break\n",
    "                \n",
    "                # Convert padded sequences back to text\n",
    "                problem_text = self.problem_tokenizer.sequences_to_texts([problem])\n",
    "                decoder_input_text = self.solution_tokenizer.sequences_to_texts([decoder_input])\n",
    "                target_text = self.solution_tokenizer.sequences_to_texts([target])\n",
    "\n",
    "                # Truncate texts\n",
    "                #max_display_length = 1024\n",
    "                #problem_text = (problem_text[:max_display_length] + '...') if len(problem_text) > max_display_length else problem_text\n",
    "                #solution_text = (solution_text[:max_display_length] + '...') if len(solution_text) > max_display_length else solution_text\n",
    "\n",
    "                # Log to TensorBoard\n",
    "                tf.summary.text(name=f\"problem_{i}\", data=problem_text, step=0)\n",
    "                tf.summary.text(name=f\"decoder_input_{i}\", data=decoder_input_text, step=0)\n",
    "                tf.summary.text(name=f\"target_{i}\", data=target_text, step=0)\n",
    "\n",
    "            self.writer.flush()\n",
    "            \n",
    "    def load_data(self):\n",
    "        problems, solutions = self._load_problems_and_solutions()\n",
    "\n",
    "        problems, decoder_inputs, targets = self.tokenize_and_pad(problems, solutions)\n",
    "        self._log_dataset_samples(problems, decoder_inputs, targets)\n",
    "        self.dataset = self._create_tf_dataset(problems, decoder_inputs, targets)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd0ef91d-0806-4046-a111-f1916d98f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3\n",
    "# Positional Encoder\n",
    "\n",
    "def positional_encoder(seq_length, dim):\n",
    "    # Generate positions for each element\n",
    "    positions = tf.range(seq_length, dtype=tf.float32)[..., tf.newaxis]\n",
    "\n",
    "    # Create a range for the dimensions and compute division terms\n",
    "    i = tf.range(dim, dtype=tf.float32)\n",
    "    div_terms = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(dim, tf.float32))\n",
    "\n",
    "    # Calculate odd/even sinusoidal encodings\n",
    "    angle_rates = positions * div_terms\n",
    "    sine = tf.sin(angle_rates[:, 0::2])\n",
    "    cosine = tf.cos(angle_rates[:, 1::2])\n",
    "\n",
    "    # Interlace and reshape\n",
    "    pos_encoding = tf.reshape(tf.concat([sine, cosine], axis=-1), [1, seq_length, dim])\n",
    "\n",
    "    return pos_encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d505a2d-a02c-4bc5-9ca5-7555f9418fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4\n",
    "# Encoder/Decoder Layer classes\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, dim_ff, key_dim, num_heads, dropout_rate, name=\"EncoderLayer\"):\n",
    "        super(EncoderLayer, self).__init__(name=name)\n",
    "\n",
    "        # Multi-Head Self-Attention layer\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "\n",
    "        # Feed-Forward Network Layers\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim_ff, kernel_initializer='he_normal', name=\"encoder_ffn_dense1\"), \n",
    "            tf.keras.layers.LeakyReLU(alpha=0.01), # Trying LeakyReLu\n",
    "            tf.keras.layers.Dense(dim, kernel_initializer='he_normal', name=\"encoder_ffn_dense2\")\n",
    "        ], name=\"encoder_ffn\")\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"encoder_layernorm1\")\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"encoder_layernorm2\")\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_mha = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # Self-Attention\n",
    "        attn_output = self.mha(x, x)\n",
    "        attn_output = self.dropout_mha(attn_output, training=training) # Dropout\n",
    "        out1 = self.layernorm1(x + attn_output)  # Residual connection\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training) # Dropout\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection\n",
    "\n",
    "        return out2\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(EncoderLayer, self).get_config()\n",
    "        mha_config = self.mha.get_config()\n",
    "        config.update({\n",
    "            \"dim\": self.ffn.layers[2].units, \n",
    "            \"dim_ff\": self.ffn.layers[0].units, \n",
    "            \"num_heads\": mha_config['num_heads'], \n",
    "            \"key_dim\": mha_config['key_dim'], \n",
    "            \"dropout_rate\": self.dropout_mha.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, dim_ff, key_dim, num_heads, dropout_rate, name=\"DecoderLayer\"):\n",
    "        super(DecoderLayer, self).__init__(name=name)\n",
    "\n",
    "        # Self-Attention and Cross-Attention layers\n",
    "        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "\n",
    "        # Feed Forward Network Layers\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim_ff, kernel_initializer='he_normal', name=\"decoder_ffn_dense1\"), \n",
    "            tf.keras.layers.LeakyReLU(alpha=0.01), # Trying LeakyReLu\n",
    "            tf.keras.layers.Dense(dim, kernel_initializer='he_normal', name=\"decoder_ffn_dense2\")\n",
    "        ], name=\"decoder_ffn\")\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm1\")\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm2\")\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm3\")\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_self_attn = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout_cross_attn = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        # Self-Attention\n",
    "        attn1_output = self.mha1(x, x, attention_mask=look_ahead_mask)\n",
    "        attn1_output = self.dropout_self_attn(attn1_output, training=training) # Dropout\n",
    "        out1 = self.layernorm1(x + attn1_output)  # Residual connection\n",
    "\n",
    "        # Cross-Attention\n",
    "        attn2_output = self.mha2(out1, enc_output, attention_mask=padding_mask)\n",
    "        attn2_output = self.dropout_cross_attn(attn2_output, training=training) # Dropout\n",
    "        out2 = self.layernorm2(out1 + attn2_output)  # Residual connection\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training) # Dropout\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # Residual connection\n",
    "\n",
    "        return out3\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DecoderLayer, self).get_config()\n",
    "        mha_config = self.mha1.get_config()  # Assumes mha1 and mha2 have the same configuration\n",
    "        config.update({\n",
    "            'dim': self.ffn.layers[2].units,\n",
    "            'dim_ff': self.ffn.layers[0].units,\n",
    "            'num_heads': mha_config['num_heads'],\n",
    "            'key_dim': mha_config['key_dim'],\n",
    "            'dropout_rate': self.dropout_self_attn.rate\n",
    "        })\n",
    "        return config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bcb9091-5ff0-4c54-89c8-ee0a5463416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5\n",
    "# Transformer\n",
    "\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"TransformerEncoder\"):\n",
    "        super(TransformerEncoder, self).__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [EncoderLayer(dim, dim_ff, key_dim, num_heads, dropout_rate, name=f\"encoder_layer_{i}\") for i in range(num_layers)]\n",
    "\n",
    "    def call(self, x: tf.Tensor, training=False) -> tf.Tensor:\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"TransformerDecoder\"):\n",
    "        super(TransformerDecoder, self).__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.dec_layers = [DecoderLayer(dim, dim_ff, key_dim, num_heads, dropout_rate, name=f\"decoder_layer_{i}\") for i in range(num_layers)]\n",
    "\n",
    "    def call(self, x: tf.Tensor, enc_output: tf.Tensor, training=False) -> tf.Tensor:\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, enc_output, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, dim, dim_ff, key_dim, problem_vocab_size, solution_vocab_size, num_heads, num_layers, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Separate embedding for input and output\n",
    "        self.problem_embedding_layer = tf.keras.layers.Embedding(problem_vocab_size, dim)\n",
    "        self.solution_embedding_layer = tf.keras.layers.Embedding(solution_vocab_size, dim)\n",
    "\n",
    "        self.encoder = TransformerEncoder(dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"encoder\")\n",
    "        self.decoder = TransformerDecoder(dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"decoder\")\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(solution_vocab_size, name=\"output_layer\")\n",
    "\n",
    "    def call(self, encoder_input, decoder_input, training=False):\n",
    "        encoder_emb = self.problem_embedding_layer(encoder_input)\n",
    "        decoder_emb = self.solution_embedding_layer(decoder_input)\n",
    "\n",
    "        seq_length_enc = tf.shape(encoder_input)[1]\n",
    "        seq_length_dec = tf.shape(decoder_input)[1]\n",
    "        pos_encoding_enc = positional_encoder(seq_length_enc, self.dim)\n",
    "        pos_encoding_dec = positional_encoder(seq_length_dec, self.dim)\n",
    "\n",
    "        encoder_emb += pos_encoding_enc\n",
    "        decoder_emb += pos_encoding_dec\n",
    "\n",
    "        encoder_output = self.encoder(encoder_emb, training=training)\n",
    "        decoder_output = self.decoder(decoder_emb, encoder_output, training=training)\n",
    "\n",
    "        final_output = self.final_layer(decoder_output)\n",
    "\n",
    "        return final_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc04145d-725e-4db0-a9a6-ab2682e60890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6\n",
    "# Build and Compile\n",
    "\n",
    "def build_and_compile(dim, dim_ff, key_dim, nhead, num_layers, problem_vocab_size, solution_vocab_size, dropout_rate, learning_rate=1e-4):\n",
    "    # Define model inputs\n",
    "    encoder_input = tf.keras.Input(shape=(None,), dtype='int32', name='encoder_input')\n",
    "    decoder_input = tf.keras.Input(shape=(None,), dtype='int32', name='decoder_input')\n",
    "\n",
    "    # Initialize and call the Transformer\n",
    "    transformer = Transformer(dim, dim_ff, key_dim, problem_vocab_size, solution_vocab_size, nhead, num_layers, dropout_rate)\n",
    "    final_output = transformer(encoder_input, decoder_input)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=final_output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        run_eagerly=True # !RUNNING EAGERLY!\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c79e4070-2bbe-423d-8228-4e9aa08347d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7\n",
    "# Define Training Steps\n",
    "\n",
    "def calculate_loss(model_output, tokenized_code, mask):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(tokenized_code, model_output, from_logits=True)\n",
    "    loss *= mask  # Apply mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, tokenized_question, tokenized_code, clip_norm=10.0):\n",
    "    with tf.GradientTape() as tape:\n",
    "        model_output = model([tokenized_question, tokenized_code], training=True)\n",
    "\n",
    "        # Mask PAD tokens\n",
    "        mask = tf.cast(tf.math.logical_not(tf.math.equal(tokenized_code, 0)), dtype=model_output.dtype)\n",
    "        \n",
    "        # Calculate loss\n",
    "        average_loss = calculate_loss(model_output, tokenized_code, mask)\n",
    "\n",
    "    # Compute and clip gradients\n",
    "    gradients = tape.gradient(average_loss, model.trainable_variables)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm)\n",
    "\n",
    "    # Apply gradients to update model weights\n",
    "    optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n",
    "\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d03c601c-7456-48fe-8bbc-bbef9a8a469d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 18:57:34.311973: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fe707a97d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-04 18:57:34.312010: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4080, Compute Capability 8.9\n",
      "2024-02-04 18:57:34.318464: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-04 18:57:34.335209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-02-04 18:57:34.433668: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f9364d3bf40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f9364d3bf40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "17/17 [==============================] - 27s 753ms/step - loss: 8.4073 - accuracy: 0.0656\n",
      "Epoch 2/2\n",
      "17/17 [==============================] - 11s 646ms/step - loss: 6.7551 - accuracy: 0.0763\n",
      "INFO:tensorflow:Assets written to: /workspace/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /workspace/assets\n"
     ]
    }
   ],
   "source": [
    "# Block 8\n",
    "# Execution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to data\n",
    "    problems_path = \"/workspace/Training_Data/A_Problems.json\"\n",
    "    submissions_dir = \"/workspace/Training_Data/A_Submissions\"\n",
    "\n",
    "    #problems_path = \"C:/AIClub/Training_Data_Cleaned/A_Problems.json\"\n",
    "    #submissions_dir = \"C:/AIClub/Training_Data_Cleaned/A_Submissions\"\n",
    "    #You'll need to update logging in Block 1\n",
    "\n",
    "    # Set hyperparameters\n",
    "    dim = 256\n",
    "    dim_ff = dim * 4\n",
    "    num_layers = 6\n",
    "    num_heads = 4\n",
    "    key_dim = dim // num_heads\n",
    "\n",
    "    max_length_input = 530 # Set to cover about 85% of inputs\n",
    "    max_length_output = 50 # Set to cover 100% of ground truth outputs\n",
    "    dropout_rate = 0.1\n",
    "\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.0001\n",
    "    epochs = 8\n",
    "\n",
    "    assert dim % num_heads == 0, \"dim % num_heads != 0\"\n",
    "\n",
    "    # Initialize the Loader\n",
    "    loader = Loader(problems_path, submissions_dir, loader_log_dir, max_length_input, max_length_output, batch_size)\n",
    "    loader.load_data()\n",
    "    problem_vocab_size = len(loader.problem_tokenizer.word_index) + 1\n",
    "    solution_vocab_size = len(loader.solution_tokenizer.word_index) + 1\n",
    "    \n",
    "    # Build the model\n",
    "    model = build_and_compile(dim, dim_ff, key_dim, num_heads, num_layers, problem_vocab_size, solution_vocab_size, dropout_rate, learning_rate)\n",
    "\n",
    "    # Setup TensorBoard callback\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=fit_log_dir, histogram_freq=1)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(loader.dataset, epochs=epochs, callbacks=[tensorboard_callback]) # history variable unused...\n",
    "    \n",
    "    \"\"\"\n",
    "    Manual training setup\n",
    "    \n",
    "    optimizer = model.optimizer\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Start of Epoch {epoch+1}\")\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, ((tokenized_question, tokenized_code), target) in enumerate(loader.dataset):\n",
    "            # Call the custom train_step\n",
    "            loss = train_step(model, optimizer, tokenized_question, tokenized_code[:, :-1])\n",
    "\n",
    "            # Log every 200 batches\n",
    "            if step % 200 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step}, Loss: {loss.numpy()}\")\n",
    "\n",
    "        print(f\"End of Epoch {epoch+1}, Loss: {loss.numpy()}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(\"/workspace\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "112669f0-144f-4c82-ae0d-9d5f93ee4fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 615ms/step\n",
      "Sample 1:\n",
      "Input sequence [:15]: tf.Tensor(\n",
      "[  39  399 1246  181 1156  348  904 1337  905   69 1338    7 1339   68\n",
      "   25], shape=(15,), dtype=int32)\n",
      "Predicted sequence: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Predicted text:                                                   \n",
      "\n",
      "Sample 2:\n",
      "Input sequence [:15]: tf.Tensor(\n",
      "[  39   32   16   78   38   37   51    5 1747  670  524    7    3  151\n",
      "   90], shape=(15,), dtype=int32)\n",
      "Predicted sequence: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Predicted text:                                                   \n",
      "\n",
      "Sample 3:\n",
      "Input sequence [:15]: tf.Tensor([ 39  32  16  78 121 233  37 195 211   5 363 133   1 109  98], shape=(15,), dtype=int32)\n",
      "Predicted sequence: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5]\n",
      "Predicted text:                                                   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Block 10\n",
    "# Evaluation Class\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, loader):\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "\n",
    "    def plot_loss(self, history):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.title('Loss Curve')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_token_probabilities(self, token_index, n_samples=1):\n",
    "        # Take one batch from the dataset\n",
    "        for (encoder_input, decoder_input), _ in self.loader.dataset.take(1):\n",
    "            # Slice the batch down to n_samples\n",
    "            encoder_input = encoder_input[:n_samples]\n",
    "            decoder_input = decoder_input[:n_samples]\n",
    "\n",
    "            # Predict on the sliced inputs\n",
    "            predictions = self.model.predict([encoder_input, decoder_input])\n",
    "\n",
    "            for sample_idx in range(n_samples):\n",
    "                # For each sample, extract token logits and convert to probabilities\n",
    "                token_logits = predictions[sample_idx, token_index, :]\n",
    "                token_probabilities = tf.nn.softmax(token_logits).numpy()\n",
    "                sorted_indices = np.argsort(token_probabilities)[::-1]\n",
    "                sorted_probabilities = token_probabilities[sorted_indices]\n",
    "\n",
    "                # Plotting\n",
    "                plt.figure(figsize=(20, 5))\n",
    "                plt.bar(range(len(sorted_probabilities)), sorted_probabilities)\n",
    "                plt.xlabel('Word Indices (sorted by probability)')\n",
    "                plt.ylabel('Probability')\n",
    "                plt.title(f'Word Prediction Probabilities for Token {token_index} in Sample {sample_idx+1}')\n",
    "                plt.show()\n",
    "\n",
    "    def generate_training_predictions(self, n_samples=1):\n",
    "        # To store prediction texts\n",
    "        predictions_texts = []\n",
    "\n",
    "        # Take one batch from the dataset\n",
    "        for (encoder_inputs, decoder_inputs), _ in self.loader.dataset.take(1):\n",
    "            # Slice the batch down to n_samples\n",
    "            encoder_inputs = encoder_inputs[:n_samples]\n",
    "            decoder_inputs = decoder_inputs[:n_samples]\n",
    "\n",
    "            # Predict on the sliced inputs\n",
    "            predictions = self.model.predict([encoder_inputs, decoder_inputs])\n",
    "            predicted_sequences = np.argmax(predictions, axis=-1)\n",
    "\n",
    "            # Convert sequences to text\n",
    "            predicted_texts = self.loader.solution_tokenizer.sequences_to_texts(predicted_sequences)\n",
    "            \n",
    "            # Print each prediction in the slice\n",
    "            for i, predicted_text in enumerate(predicted_texts):\n",
    "                print(f\"Sample {i + 1}:\") \n",
    "                print(\"Input sequence [:15]:\", encoder_inputs[i][:15])\n",
    "                print(\"Predicted sequence:\", predicted_sequences[i])\n",
    "                print(\"Predicted text:\", predicted_text, \"\\n\")\n",
    "    \n",
    "    def generate_manual_predictions(self, input_text):\n",
    "        # Tokenize the input string\n",
    "        input_seq = self.loader.problem_tokenizer.texts_to_sequences([input_text])\n",
    "        input_padded = tf.keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=self.loader.max_length_input, padding='post')\n",
    "\n",
    "        # Prepare the decoder input\n",
    "        start_token_index = self.loader.solution_tokenizer.word_index.get('[START]', 1)  # Fallback to 1 if not found\n",
    "        decoder_input = np.array([[start_token_index]])\n",
    "        \n",
    "        # Generate and interpret the prediction\n",
    "        predictions = self.model.predict([input_padded, decoder_input])\n",
    "        predicted_sequence = np.argmax(predictions, axis=-1)[0]\n",
    "        predicted_text = self.loader.solution_tokenizer.sequences_to_texts([predicted_sequence])\n",
    "        \n",
    "        # Print the output\n",
    "        print(\"Input text:\", input_text)\n",
    "        print(\"Predicted text:\", predicted_text[0])\n",
    "\n",
    "    def evaluate(self, command, *args, **kwargs):\n",
    "        if command == 'loss':\n",
    "            self.plot_loss(*args, **kwargs)\n",
    "        elif command == 'token_prob':\n",
    "            self.plot_token_probabilities(*args, **kwargs)\n",
    "        elif command == 'training_sample_pred':\n",
    "            self.generate_training_predictions(*args, **kwargs)\n",
    "        elif command == 'manual_sample_pred':\n",
    "            self.generate_manual_predictions(*args, **kwargs)\n",
    "        else:\n",
    "            print(f\"Unknown command: {command}\")\n",
    "\n",
    "# Load the model if it's not\n",
    "model = tf.keras.models.load_model('/workspace')\n",
    "\n",
    "# Uncomment what you want to run\n",
    "evaluator = Evaluator(model, loader)\n",
    "#evaluator.evaluate('loss', history)\n",
    "#evaluator.evaluate('token_prob', token_index=10, n_samples=3)\n",
    "evaluator.evaluate('training_sample_pred', n_samples=3)\n",
    "\n",
    "input_text = \"Print 'hello world' followed by 2*n evaluations\"\n",
    "#evaluator.evaluate('manual_sample_pred', input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63cd5725-9961-414f-a052-207233613908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c1cf03c5257e5b6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c1cf03c5257e5b6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8088;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --host localhost --port 8088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa2755c7-0765-457e-8843-b235dc3a30df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(len(loader.problem_tokenizer.word_index) + 1)\n",
    "#print(len(loader.solution_tokenizer.word_index) + 1)\n",
    "\n",
    "# Clears logs folder\n",
    "#!find logs -mindepth 1 -delete\n",
    "\n",
    "#tf.debugging.experimental.disable_dump_debug_info()\n",
    "\n",
    "#!kill 415\n",
    "\n",
    "#print(\"hello\")\n",
    "\n",
    "#print(tf.sysconfig.get_build_info())\n",
    "\n",
    "#tf.__version__\n",
    "#!tensorboard --version\n",
    "\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6dbb14-f316-47e2-a8a3-16faaf790ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196b8c1-fa3c-422e-be19-bea76c8681f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
