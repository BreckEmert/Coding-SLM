{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c8447c-afb5-46b5-9b44-023332d5d296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 16:55:52.785641: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-22 16:55:52.785700: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-22 16:55:52.785723: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-22 16:55:52.791512: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Block 1\n",
    "# Imports\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorboard\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3532f1-59c9-4421-83cb-49c9dd54bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2\n",
    "# Loader\n",
    "\n",
    "\n",
    "class Loader:\n",
    "    def __init__(self, problems_path, submissions_dir, log_dir, max_length, batch_size):\n",
    "        self.problems_path = problems_path\n",
    "        self.submissions_dir = submissions_dir\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.tokenizer = Tokenizer(filters='')\n",
    "        self.dataset = None\n",
    "        \n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    def _load_problems(self):\n",
    "        with open(self.problems_path, 'r') as f:\n",
    "            problems_list = json.load(f)\n",
    "\n",
    "        problems = {}\n",
    "        for problem in problems_list:\n",
    "            problem_id = problem['problem_id']\n",
    "            concatenated_problem = \"XXSTATEMENT {} XXINPUT {} XXOUTPUT {} XXNOTES {} XXEXAMPLES {}\".format(\n",
    "                problem.get('problem_statement', ''),\n",
    "                problem.get('problem_input', ''),\n",
    "                problem.get('problem_output', ''),\n",
    "                problem.get('problem_notes', ''),\n",
    "                problem.get('examples', '')\n",
    "            )\n",
    "            problems[problem_id] = concatenated_problem\n",
    "\n",
    "        return problems\n",
    "\n",
    "    def _load_solutions(self, problems):\n",
    "        # Appends solutions to problems\n",
    "        solutions = []\n",
    "        for filepath in glob.glob(os.path.join(self.submissions_dir, \"*.py\")):\n",
    "            problem_number = int(re.findall(r'^\\d+', os.path.basename(filepath))[0])\n",
    "            if problem_number in problems:\n",
    "                with open(filepath, \"r\") as f:\n",
    "                    solutions.append((problems[problem_number], f.read()))\n",
    "\n",
    "        return solutions\n",
    "\n",
    "    def _tokenize_and_pad(self, problems_solutions):\n",
    "        # Tokenize and pad at the same time for efficiency\n",
    "        texts = [problem for problem, _ in problems_solutions] + [solution for _, solution in problems_solutions]\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "\n",
    "        sequences = [self.tokenizer.texts_to_sequences([text])[0] for text in texts]\n",
    "        padded_sequences = pad_sequences(sequences, padding='post', maxlen=self.max_length)\n",
    "        \n",
    "        # Split back into problems and solutions\n",
    "        midpoint = len(padded_sequences) // 2\n",
    "        problem_padded = padded_sequences[:midpoint]\n",
    "        solution_padded = padded_sequences[midpoint:]\n",
    "        \n",
    "        # Log dataset samples\n",
    "        with self.writer.as_default():\n",
    "            for i, (problem, solution) in enumerate(zip(problem_padded, solution_padded)):\n",
    "                if i >= 5:  # 5 samples\n",
    "                    break\n",
    "                # Log problem\n",
    "                problem_text = self.tokenizer.sequences_to_texts([problem])[0]\n",
    "                tf.summary.text(name=f\"Problem_{i}\", data=problem_text, step=0)\n",
    "\n",
    "                # Log solution\n",
    "                solution_text = self.tokenizer.sequences_to_texts([solution])[0]\n",
    "                tf.summary.text(name=f\"Solution_{i}\", data=solution_text, step=0)\n",
    "            \n",
    "            self.writer.flush()\n",
    "                \n",
    "        return problem_padded, solution_padded\n",
    "    \n",
    "    def _tokenize_and_pad(self, problems_solutions):\n",
    "        # Tokenize and pad at the same time for efficiency\n",
    "        texts = [problem for problem, _ in problems_solutions] + [solution for _, solution in problems_solutions]\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "\n",
    "        sequences = [self.tokenizer.texts_to_sequences([text])[0] for text in texts]\n",
    "        padded_sequences = pad_sequences(sequences, padding='post', maxlen=self.max_length)\n",
    "\n",
    "        # Split back into problems and solutions\n",
    "        mid_point = len(padded_sequences) // 2\n",
    "        return padded_sequences[:mid_point], padded_sequences[mid_point:]\n",
    "\n",
    "    def _create_tf_dataset(self, problem_padded, solution_padded):\n",
    "        # Prepare decoder input (shifted solution)\n",
    "        decoder_input = tf.pad(solution_padded, [[0, 0], [1, 0]])[:, :-1]  # Shift left\n",
    "\n",
    "        # Target is the original solution\n",
    "        target = solution_padded\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(((problem_padded, decoder_input), target))\n",
    "        return dataset.shuffle(buffer_size=1024).batch(self.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "                \n",
    "    def load_data(self):\n",
    "        problems = self._load_problems()\n",
    "        solutions = self._load_solutions(problems)\n",
    "\n",
    "        problem_padded, solution_padded = self._tokenize_and_pad(solutions)\n",
    "        self.dataset = self._create_tf_dataset(problem_padded, solution_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0ef91d-0806-4046-a111-f1916d98f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3\n",
    "# Positional Encoder\n",
    "\n",
    "def positional_encoder(seq_length, d_model):\n",
    "    # Generate positions\n",
    "    positions = tf.range(seq_length, dtype=tf.float32)[..., tf.newaxis]\n",
    "\n",
    "    # Indices for div_terms calculation\n",
    "    i = tf.range(d_model, dtype=tf.float32)\n",
    "    div_terms = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # Calculate sinusoidal encodings\n",
    "    angle_rates = positions * div_terms\n",
    "    sine = tf.sin(angle_rates[:, 0::2])\n",
    "    cosine = tf.cos(angle_rates[:, 1::2])\n",
    "\n",
    "    # Interlace\n",
    "    pos_encoding = tf.reshape(tf.concat([sine, cosine], axis=-1), [1, seq_length, d_model])\n",
    "\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d505a2d-a02c-4bc5-9ca5-7555f9418fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4\n",
    "# Encoder/Decoder Layer classes\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim_ff, dim, num_heads, dropout_rate, name=\"EncoderLayer\"):\n",
    "        super(EncoderLayer, self).__init__(name=name)\n",
    "\n",
    "        # Multi-Head Self-Attention layer\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "\n",
    "        # Feed-Forward Network Layers\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim_ff, activation='relu', kernel_initializer='he_normal', name=\"encoder_ffn_dense1\"),\n",
    "            tf.keras.layers.Dense(dim, kernel_initializer='he_normal', name=\"encoder_ffn_dense2\")\n",
    "        ], name=\"encoder_ffn\")\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"encoder_layernorm1\")\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"encoder_layernorm2\")\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_mha = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # Self-Attention\n",
    "        attn_output = self.mha(x, x)  # Self attention\n",
    "        attn_output = self.dropout_mha(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # Residual connection\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        ffn_input = self.layernorm2(out1)\n",
    "        ffn_out = self.ffn(ffn_input)\n",
    "        ffn_out = self.dropout_ffn(ffn_out, training=training)\n",
    "        out2 = out1 + ffn_out  # Residual connection\n",
    "\n",
    "        return out2\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(EncoderLayer, self).get_config()\n",
    "        mha_config = self.mha.get_config()  # Won't work if mha1 and mha2 are different\n",
    "        config.update({\n",
    "            \"dim_ff\": self.ffn.layers[0].units,\n",
    "            \"num_heads\": mha_config['num_heads'], \n",
    "            \"key_dim\": mha_config['key_dim'], \n",
    "            \"dropout_rate\": self.dropout_mha.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim_ff, dim, num_heads, dropout_rate, name=\"DecoderLayer\"):\n",
    "        super(DecoderLayer, self).__init__(name=name)\n",
    "\n",
    "        # Self-Attention and Cross-Attention layers\n",
    "        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "\n",
    "        # Feed Forward Network Layers\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim_ff, activation='relu', kernel_initializer='he_normal', name=\"decoder_ffn_dense1\"),\n",
    "            tf.keras.layers.Dense(dim, kernel_initializer='he_normal', name=\"decoder_ffn_dense2\")\n",
    "        ], name=\"decoder_ffn\")\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm1\")\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm2\")\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm3\")\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_self_attn = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout_cross_attn = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        # Self-Attention\n",
    "        attn1_output = self.mha1(x, x, attention_mask=look_ahead_mask)\n",
    "        attn1_output = self.dropout_self_attn(attn1_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn1_output)  # Residual connection\n",
    "\n",
    "        # Cross-Attention\n",
    "        attn2_output = self.mha2(out1, enc_output, attention_mask=padding_mask)\n",
    "        attn2_output = self.dropout_cross_attn(attn2_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + attn2_output)  # Residual connection\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        ffn_out = self.ffn(out2)\n",
    "        ffn_out = self.dropout_ffn(ffn_out, training=training)\n",
    "        out3 = self.layernorm3(ffn_out + out2)  # Residual connection\n",
    "\n",
    "        return out3\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DecoderLayer, self).get_config()\n",
    "        mha1_config = self.mha1.get_config()  # Won't work if mha1 and mha2 are different\n",
    "        config.update({\n",
    "            \"dim_ff\": self.ffn.layers[0].units,\n",
    "            \"num_heads\": mha1_config['num_heads'], \n",
    "            \"key_dim\": mha1_config['key_dim'], \n",
    "            \"dropout_rate\": self.dropout_self_attn.rate\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bcb9091-5ff0-4c54-89c8-ee0a5463416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5\n",
    "# Transformer\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"TransformerEncoder\"):\n",
    "        super(TransformerEncoder, self).__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [EncoderLayer(dim_ff, dim, num_heads, dropout_rate, name=f\"encoder_layer_{i}\") for i in range(num_layers)]\n",
    "\n",
    "    def call(self, x: tf.Tensor, training=False) -> tf.Tensor:\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"TransformerDecoder\"):\n",
    "        super(TransformerDecoder, self).__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.dec_layers = [DecoderLayer(dim_ff, dim, num_heads, dropout_rate, name=f\"decoder_layer_{i}\") for i in range(num_layers)]\n",
    "\n",
    "    def call(self, x: tf.Tensor, enc_output: tf.Tensor, training=False) -> tf.Tensor:\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, enc_output, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, dim, dim_ff, key_dim, vocab_size, num_heads, num_layers, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(vocab_size, dim)\n",
    "\n",
    "        self.encoder = TransformerEncoder( dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"encoder\")\n",
    "        self.decoder = TransformerDecoder( dim, dim_ff, key_dim, num_heads, num_layers, dropout_rate, name=\"decoder\")\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(vocab_size, name=\"output_layer\")\n",
    "\n",
    "    def call(self, encoder_input, decoder_input, training=False):\n",
    "        encoder_emb = self.embedding_layer(encoder_input)\n",
    "        decoder_emb = self.embedding_layer(decoder_input)\n",
    "\n",
    "        seq_length_enc = tf.shape(encoder_input)[1]\n",
    "        seq_length_dec = tf.shape(decoder_input)[1]\n",
    "        pos_encoding_enc = positional_encoder(seq_length_enc, self.dim)\n",
    "        pos_encoding_dec = positional_encoder(seq_length_dec, self.dim)\n",
    "\n",
    "        encoder_emb += pos_encoding_enc\n",
    "        decoder_emb += pos_encoding_dec\n",
    "\n",
    "        encoder_output = self.encoder(encoder_emb, training=training)\n",
    "        decoder_output = self.decoder(decoder_emb, encoder_output, training=training)\n",
    "\n",
    "        final_output = self.final_layer(decoder_output)\n",
    "\n",
    "        return final_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc04145d-725e-4db0-a9a6-ab2682e60890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6\n",
    "# Build and Compile\n",
    "\n",
    "def build_and_compile(dim, dim_ff, key_dim, nhead, num_layers, vocab_size, dropout_rate, learning_rate=1e-4):\n",
    "    # Define model inputs\n",
    "    encoder_input = tf.keras.Input(shape=(None,), dtype='int32', name='encoder_input')\n",
    "    decoder_input = tf.keras.Input(shape=(None,), dtype='int32', name='decoder_input')\n",
    "\n",
    "    # Initialize and call the Transformer\n",
    "    transformer = Transformer(dim, dim_ff, key_dim, vocab_size, nhead, num_layers, dropout_rate)\n",
    "    decoder_output = transformer(encoder_input, decoder_input)\n",
    "\n",
    "    # Final Dense layer for classification\n",
    "    outputs = tf.keras.layers.Dense(vocab_size, name='output_layer')(decoder_output)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c79e4070-2bbe-423d-8228-4e9aa08347d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7\n",
    "# Define Training Steps\n",
    "\n",
    "def calculate_loss(model_output, tokenized_code, mask):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(tokenized_code, model_output, from_logits=True)\n",
    "    loss *= mask  # Apply mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, tokenized_question, tokenized_code, clip_norm=1.0):\n",
    "    with tf.GradientTape() as tape:\n",
    "        model_output = model([tokenized_question, tokenized_code], training=True)\n",
    "\n",
    "        # Mask PAD tokens\n",
    "        mask = tf.cast(tf.math.logical_not(tf.math.equal(tokenized_code, 0)), dtype=model_output.dtype)\n",
    "        \n",
    "        # Calculate loss\n",
    "        average_loss = calculate_loss(model_output, tokenized_code, mask)\n",
    "\n",
    "    # Compute and clip gradients\n",
    "    gradients = tape.gradient(average_loss, model.trainable_variables)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm)\n",
    "\n",
    "    # Apply gradients to update model weights\n",
    "    optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d03c601c-7456-48fe-8bbc-bbef9a8a469d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 15:34:44.032533: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb2f4a64650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-22 15:34:44.032581: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 SUPER, Compute Capability 7.5\n",
      "2023-11-22 15:34:44.037554: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-22 15:34:44.049115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2023-11-22 15:34:44.128935: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 39s 39s/step - loss: 18.7948 - accuracy: 0.0000e+00\n",
      "INFO:tensorflow:Assets written to: /workspace/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /workspace/assets\n"
     ]
    }
   ],
   "source": [
    "# Block 8\n",
    "# Execution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths to data\n",
    "    problems_path = \"/workspace/Training_Data/A_Problems.json\"\n",
    "    submissions_dir = \"/workspace/Training_Data/A_Submissions_4\"\n",
    "    log_dir = \"/workspace/logs\"\n",
    "\n",
    "    # Set hyperparameters\n",
    "    dim = 256\n",
    "    dim_ff = dim * 4\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    key_dim = dim // num_heads\n",
    "\n",
    "    max_length = 530 # Set to cover about 85% of inputs\n",
    "    dropout_rate = 0.01 # Lowered temporarily\n",
    "\n",
    "    batch_size = 16\n",
    "    learning_rate = 0.002\n",
    "    epochs = 1\n",
    "\n",
    "    assert dim % num_heads == 0, \"dim % num_heads != 0\"\n",
    "\n",
    "    # Initialize the Loader\n",
    "    loader = Loader(problems_path, submissions_dir, log_dir, max_length, batch_size)\n",
    "    loader.load_data()\n",
    "    vocab_size = len(loader.tokenizer.word_index) + 1\n",
    "    \n",
    "    # Build the model\n",
    "    model = build_and_compile(dim, dim_ff, key_dim, num_heads, num_layers, vocab_size, dropout_rate, learning_rate)\n",
    "\n",
    "    # Setup TensorBoard callback\n",
    "    log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(loader.dataset, epochs=epochs, callbacks=[tensorboard_callback])\n",
    "\n",
    "    # Save the model\n",
    "    model.save(\"/workspace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112669f0-144f-4c82-ae0d-9d5f93ee4fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to synchronously open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown command: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Load the model if it's not\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/workspace/saved_model.pb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Uncomment what you want to run\u001b[39;00m\n\u001b[1;32m     63\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator(model, loader)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to synchronously open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "# Block 10\n",
    "# Evaluation Class\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, model, loader):\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "\n",
    "    def plot_loss(self, history):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.title('Loss Curve')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_token_probabilities(self, token_index, n_samples):\n",
    "        # Get n_samples from the dataset\n",
    "        for (encoder_input, decoder_input), _ in self.loader.dataset.take(n_samples):\n",
    "            prediction = self.model.predict([encoder_input, decoder_input])\n",
    "            token_logits = prediction[0, token_index]\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            token_probabilities = tf.nn.softmax(token_logits).numpy()\n",
    "\n",
    "            sorted_indices = np.argsort(token_probabilities)[::-1]\n",
    "            sorted_probabilities = token_probabilities[sorted_indices]\n",
    "\n",
    "            plt.figure(figsize=(20, 5))\n",
    "            plt.bar(range(len(sorted_probabilities)), sorted_probabilities)\n",
    "            plt.xlabel('Word Indices (sorted by probability)')\n",
    "            plt.ylabel('Probability')\n",
    "            plt.title(f'Word Prediction Probabilities for Token {token_index}')\n",
    "            plt.show()\n",
    "\n",
    "    def generate_sample_predictions(self, n_samples=1):\n",
    "        # Get n_samples from the dataset\n",
    "        for (encoder_input, decoder_input), _ in self.loader.dataset.take(n_samples):\n",
    "            print(encoder_input[1][:10])\n",
    "            print(decoder_input[1][:10])\n",
    "            prediction = self.model.predict([encoder_input, decoder_input])\n",
    "            predicted_sequence = np.argmax(prediction, axis=-1)\n",
    "            predicted_text = self.loader.tokenizer.sequences_to_texts(predicted_sequence)\n",
    "\n",
    "            print(\"Predicted sequence: \", predicted_sequence)\n",
    "            print(\"Predicted text: \", predicted_text)\n",
    "\n",
    "    def evaluate(self, command, *args, **kwargs):\n",
    "        if command == 'loss':\n",
    "            self.plot_loss(*args, **kwargs)\n",
    "        elif command == 'token_prob':\n",
    "            self.plot_token_probabilities(*args, **kwargs)\n",
    "        elif command == 'sample_pred':\n",
    "            self.generate_sample_predictions(*args, **kwargs)\n",
    "        else:\n",
    "            print(f\"Unknown command: {command}\")\n",
    "\n",
    "# Load the model if it's not\n",
    "model = tf.keras.models.load_model('/workspace/saved_model.pb')\n",
    "\n",
    "# Uncomment what you want to run\n",
    "evaluator = Evaluator(model, loader)\n",
    "#evaluator.evaluate('loss', history)\n",
    "#evaluator.evaluate('token_prob', token_index=10, n_samples=4)\n",
    "evaluator.evaluate('sample_pred', n_samples=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63cd5725-9961-414f-a052-207233613908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-22 16:19:16.422553: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-22 16:19:16.422617: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-22 16:19:16.422668: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-22 16:19:16.427938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-22 16:19:17.936294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:05:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 16:19:17.940741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:05:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-11-22 16:19:17.940828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:05:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.14.1 at http://1519d39aef72:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir logs --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3fe4a14-9e47-49f1-ba76-b33996e4e827",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rapids/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
