{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a4ee0c-ada7-4545-bade-b322c23609ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from SLM import Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e4af3-6d34-419a-a61d-dc816a4ff825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Loaders:\n",
    "    def __init__:\n",
    "        base_log_dir = os.path.join(\"/workspace/logs\", \"run_\" + datetime.datetime.now().strftime(\"%m_%d_%H_%M\"))\n",
    "        loader_log_dir = os.path.join(base_log_dir, \"loader\")\n",
    "        \n",
    "    def load_CodeForces_A_difficulty:\n",
    "        # Load problems\n",
    "        with open(self.problems_path, 'r') as problems_file:\n",
    "            problems_list = json.load(problems_file)\n",
    "        raw_problems = {}\n",
    "\n",
    "        for problem in problems_list:\n",
    "            problem_id = problem['problem_id']\n",
    "            concatenated_problem = \"XXSTATEMENT {} XXINPUT {} XXOUTPUT {} XXNOTES {} XXEXAMPLES {}\".format(\n",
    "                problem.get('problem_statement', ''),\n",
    "                problem.get('problem_input', ''),\n",
    "                problem.get('problem_output', ''),\n",
    "                problem.get('problem_notes', ''),\n",
    "                problem.get('examples', '')\n",
    "            )\n",
    "            raw_problems[problem_id] = concatenated_problem\n",
    "\n",
    "        # Load solutions\n",
    "        raw_solutions = [[] for _ in range(515)] # 515 python submissions\n",
    "        submissions = glob.glob(os.path.join(self.submissions_dir, \"*.py\"))\n",
    "\n",
    "        for submission_path in submissions:\n",
    "            problem_number = int(re.findall(r'^\\d+', os.path.basename(submission_path))[0])\n",
    "            with open(submission_path, \"r\") as submission:\n",
    "                raw_solutions[problem_number].append(submission.read())\n",
    "\n",
    "        # Combine problems and solutions\n",
    "        problems = []\n",
    "        solutions = []\n",
    "        for problem_id, solution_set in enumerate(raw_solutions):\n",
    "            if solution_set:\n",
    "                for solution in solution_set:\n",
    "                    problems.append(raw_problems[problem_id])\n",
    "                    solutions.append(solution)\n",
    "\n",
    "        # Tokenize and pad\n",
    "        problems = tokenizer.tokenize_input(problems)\n",
    "        decoder_inputs, targets = tokenizer.tokenize_output(solutions)\n",
    "        \n",
    "    def load_ProblemSolutionPythonV3():\n",
    "        df = pd.read_csv(self.csv_path)\n",
    "\n",
    "        # Initialize problems and solutions lists\n",
    "        problems = []\n",
    "        solutions = []\n",
    "\n",
    "        # Iterate over the DataFrame rows\n",
    "        for index, row in df.iterrows():\n",
    "            problem = row['Problem']\n",
    "            solution = row['Python Code']\n",
    "            problems.append(problem)\n",
    "            solutions.append(solution)\n",
    "\n",
    "        # Tokenize and pad\n",
    "        problems, decoder_inputs, targets = tokenizer.tokenize(problems, solutions)\n",
    "    \n",
    "    def write_file(problems, decoder_inputs, targets, output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filepath = os.path.join(output_dir, f\"tokenized_padded_data.npz\")\n",
    "        np.savez_compressed(filepath, problems=problems, decoder_inputs=decoder_inputs, targets=targets)\n",
    "        \n",
    "    def load_data(dataset_path):\n",
    "        match dataset_path:\n",
    "            case \"/workspace/Training_Data/CodeForces_A_difficulty\":\n",
    "                load_CodeForces_A_difficulty()\n",
    "\n",
    "            case \"/workspace/Training_Data/ProblemSolutionV3\":\n",
    "                load_ProblemSolutionPythonV3()\n",
    "\n",
    "            case \"All\"\n",
    "                load_CodeForces_A_difficulty()\n",
    "                load_ProblemSolutionPythonV3()\n",
    "\n",
    "                write_file(both)\n",
    "            \n",
    "            case _:\n",
    "                \"Invalid dataset\"\n",
    "        \n",
    "        write_file(problems, decoder_inputs, targets, dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
