{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1c8447c-afb5-46b5-9b44-023332d5d296",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 02:47:48.231601: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-08 02:47:48.459856: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-08 02:47:48.459878: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-08 02:47:48.461137: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-08 02:47:48.561767: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Enable full debugging\\n#tf.config.optimizer.set_jit(False)  # Disable XLA compilation\\ntf.config.run_functions_eagerly(True)\\ntf.data.experimental.enable_debug_mode() # Disables tf.data eager execution; not covered by run_functions_eagerly\\n\\ntf.debugging.experimental.enable_dump_debug_info(\\n    debug_log_dir,\\n    tensor_debug_mode=\"NO_TENSOR\", # CONCISE_HEALTH\\n    circular_buffer_size=1000) # 1000\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from Transformer import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd3532f1-59c9-4421-83cb-49c9dd54bad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(self, dataset_path, args):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.dataset = None\n",
    "\n",
    "    def create_dataset(self, problems, decoder_inputs, targets):\n",
    "        # Ensure data exists\n",
    "        if not os.path.exists(self.dataset_path):\n",
    "            transform_raw_data.load_data(self.dataset_path)\n",
    "        \n",
    "        # Load the .npz file\n",
    "        data = np.load(self.dataset_path)\n",
    "        problems = data['problems']\n",
    "        decoder_inputs = data['decoder_inputs']\n",
    "        targets = data['targets']\n",
    "        \n",
    "        # Create the dataset\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices(((problems, decoder_inputs), targets))\n",
    "        self.dataset.shuffle(buffer_size=1024).batch(args.batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd0ef91d-0806-4046-a111-f1916d98f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoder(seq_length, dim):\n",
    "    # Generate positions for each element\n",
    "    positions = tf.range(seq_length, dtype=tf.float32)[..., tf.newaxis]\n",
    "    \n",
    "    # Create a range for the dimensions and compute division terms\n",
    "    i = tf.range(dim, dtype=tf.float32)\n",
    "    div_terms = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(dim, tf.float32))\n",
    "    \n",
    "    # Calculate odd/even sinusoidal encodings\n",
    "    angle_rates = positions * div_terms\n",
    "    sine = tf.sin(angle_rates[:, 0::2])\n",
    "    cosine = tf.cos(angle_rates[:, 1::2])\n",
    "    \n",
    "    # Interlace and reshape\n",
    "    pos_encoding = tf.reshape(tf.concat([sine, cosine], axis=-1), [1, seq_length, dim])\n",
    "    \n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d505a2d-a02c-4bc5-9ca5-7555f9418fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, args, name=\"EncoderLayer\"):\n",
    "        super(EncoderLayer, self).__init__(name=name)\n",
    "\n",
    "        # Multi-Head Self-Attention layer\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=args.num_heads, key_dim=args.key_dim)\n",
    "\n",
    "        # Feed-Forward Network Layers\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(args.dim_ff, kernel_initializer='he_normal', name=\"encoder_ffn_dense1\"), \n",
    "            tf.keras.layers.LeakyReLU(alpha=0.01), # Trying LeakyReLu\n",
    "            tf.keras.layers.Dense(args.dim, kernel_initializer='he_normal', name=\"encoder_ffn_dense2\")\n",
    "        ], name=\"encoder_ffn\")\n",
    "\n",
    "        # Normalization Layers\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"encoder_layernorm1\")\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"encoder_layernorm2\")\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout_mha = tf.keras.layers.Dropout(args.dropout_rate)\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(args.dropout_rate)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        # Self-Attention\n",
    "        attn_output = self.mha(x, x)\n",
    "        attn_output = self.dropout_mha(attn_output, training=training) # Dropout\n",
    "        out1 = self.layernorm1(x + attn_output)  # Residual connection\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training) # Dropout\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection\n",
    "\n",
    "        return out2\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(EncoderLayer, self).get_config()\n",
    "        mha_config = self.mha.get_config()\n",
    "        config.update({\n",
    "            \"dim\": self.ffn.layers[2].units, \n",
    "            \"dim_ff\": self.ffn.layers[0].units, \n",
    "            \"num_heads\": mha_config['num_heads'], \n",
    "            \"key_dim\": mha_config['key_dim'], \n",
    "            \"dropout_rate\": self.dropout_mha.rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, args, name=\"DecoderLayer\"):\n",
    "        super(DecoderLayer, self).__init__(name=name)\n",
    "        \n",
    "        # Self-Attention and Cross-Attention layers\n",
    "        self.mha1 = tf.keras.layers.MultiHeadAttention(num_heads=args.num_heads, key_dim=args.key_dim)\n",
    "        self.mha2 = tf.keras.layers.MultiHeadAttention(num_heads=args.num_heads, key_dim=args.key_dim)\n",
    "        \n",
    "        # Feed Forward Network Layers\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim_ff, kernel_initializer='he_normal', name=\"decoder_ffn_dense1\"), \n",
    "            tf.keras.layers.LeakyReLU(alpha=0.01), # Trying LeakyReLu\n",
    "            tf.keras.layers.Dense(dim, kernel_initializer='he_normal', name=\"decoder_ffn_dense2\")\n",
    "        ], name=\"decoder_ffn\")\n",
    "        \n",
    "        # Normalization Layers\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm1\")\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm2\")\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"decoder_layernorm3\")\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout_self_attn = tf.keras.layers.Dropout(args.dropout_rate)\n",
    "        self.dropout_cross_attn = tf.keras.layers.Dropout(args.dropout_rate)\n",
    "        self.dropout_ffn = tf.keras.layers.Dropout(args.dropout_rate)\n",
    "\n",
    "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
    "        # Self-Attention\n",
    "        attn1_output = self.mha1(x, x, attention_mask=look_ahead_mask)\n",
    "        attn1_output = self.dropout_self_attn(attn1_output, training=training) # Dropout\n",
    "        out1 = self.layernorm1(x + attn1_output)  # Residual connection\n",
    "        \n",
    "        # Cross-Attention\n",
    "        attn2_output = self.mha2(out1, enc_output, attention_mask=padding_mask)\n",
    "        attn2_output = self.dropout_cross_attn(attn2_output, training=training) # Dropout\n",
    "        out2 = self.layernorm2(out1 + attn2_output)  # Residual connection\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training) # Dropout\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # Residual connection\n",
    "        \n",
    "        return out3\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DecoderLayer, self).get_config()\n",
    "        mha_config = self.mha1.get_config()  # Assumes mha1 and mha2 have the same configuration\n",
    "        config.update({\n",
    "            'dim': self.ffn.layers[2].units,\n",
    "            'dim_ff': self.ffn.layers[0].units,\n",
    "            'num_heads': mha_config['num_heads'],\n",
    "            'key_dim': mha_config['key_dim'],\n",
    "            'dropout_rate': self.dropout_self_attn.rate\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bcb9091-5ff0-4c54-89c8-ee0a5463416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, args, name=\"TransformerEncoder\"):\n",
    "        super(TransformerEncoder, self).__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = [EncoderLayer(args, name=f\"encoder_layer_{i}\") for i in range(num_layers)]\n",
    "\n",
    "    def call(self, x: tf.Tensor, training=False) -> tf.Tensor:\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, args, name=\"TransformerDecoder\"):\n",
    "        super(TransformerDecoder, self).__init__(name=name)\n",
    "        self.num_layers = num_layers\n",
    "        self.dec_layers = [DecoderLayer(dim, dim_ff, key_dim, num_heads, dropout_rate, name=f\"decoder_layer_{i}\") for i in range(num_layers)]\n",
    "\n",
    "    def call(self, x: tf.Tensor, enc_output: tf.Tensor, training=False) -> tf.Tensor:\n",
    "        for layer in self.dec_layers:\n",
    "            x = layer(x, enc_output, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, args):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Separate embedding for input and output\n",
    "        self.problem_embedding_layer = tf.keras.layers.Embedding(args.problem_vocab_size, args.dim, mask_zero=True)\n",
    "        self.solution_embedding_layer = tf.keras.layers.Embedding(args.solution_vocab_size, args.dim, mask_zero=True)\n",
    "\n",
    "        self.encoder = TransformerEncoder(args, name=\"encoder\")\n",
    "        self.decoder = TransformerDecoder(args, name=\"decoder\")\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(args.solution_vocab_size, name=\"output_layer\")\n",
    "\n",
    "    def call(self, encoder_input, decoder_input, training=False):\n",
    "        # Embed input sequences\n",
    "        encoder_emb = self.problem_embedding_layer(encoder_input)\n",
    "        decoder_emb = self.solution_embedding_layer(decoder_input)\n",
    "        \n",
    "        # Generate and add positional encodings\n",
    "        seq_length_enc = tf.shape(encoder_input)[1]\n",
    "        seq_length_dec = tf.shape(decoder_input)[1]\n",
    "        pos_encoding_enc = positional_encoder(seq_length_enc, self.dim)\n",
    "        pos_encoding_dec = positional_encoder(seq_length_dec, self.dim)\n",
    "        \n",
    "        encoder_emb += pos_encoding_enc\n",
    "        decoder_emb += pos_encoding_dec\n",
    "        \n",
    "        # Forward pass\n",
    "        encoder_output = self.encoder(encoder_emb, training=training)\n",
    "        decoder_output = self.decoder(decoder_emb, encoder_output, training=training)\n",
    "\n",
    "        final_output = self.final_layer(decoder_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc04145d-725e-4db0-a9a6-ab2682e60890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile(args: ModelArgs): # Do we only call args here or what\n",
    "    # Define model inputs\n",
    "    encoder_input = tf.keras.Input(shape=(None,), dtype='int32', name='encoder_input')\n",
    "    decoder_input = tf.keras.Input(shape=(None,), dtype='int32', name='decoder_input')\n",
    "\n",
    "    # Initialize and call the Transformer\n",
    "    transformer = Transformer(args)\n",
    "    final_output = transformer(encoder_input, decoder_input)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=[encoder_input, decoder_input], outputs=final_output)\n",
    "\n",
    "    # Compile the model\n",
    "    #lr_schedule = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.005, decay_steps=500, alpha=0.0001)\n",
    "    # dataset size / batch size times epochs, is time until decay to alpha\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=args.learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(ignore_class=0, from_logits=True),\n",
    "        metrics=['accuracy'],\n",
    "        run_eagerly=False # !CAUTION!\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c79e4070-2bbe-423d-8228-4e9aa08347d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(model_output, tokenized_code, mask):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(tokenized_code, model_output, from_logits=True)\n",
    "    loss *= mask  # Apply mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, optimizer, tokenized_question, tokenized_code, clip_norm=10.0):\n",
    "    with tf.GradientTape() as tape:\n",
    "        model_output = model([tokenized_question, tokenized_code], training=True)\n",
    "\n",
    "        # Mask PAD tokens\n",
    "        mask = tf.cast(tf.math.logical_not(tf.math.equal(tokenized_code, 0)), dtype=model_output.dtype)\n",
    "        \n",
    "        # Calculate loss\n",
    "        average_loss = calculate_loss(model_output, tokenized_code, mask)\n",
    "\n",
    "    # Compute and clip gradients\n",
    "    gradients = tape.gradient(average_loss, model.trainable_variables)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm)\n",
    "\n",
    "    # Apply gradients to update model weights\n",
    "    optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa2755c7-0765-457e-8843-b235dc3a30df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.14.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(loader.problem_tokenizer.word_index) + 1)\n",
    "#print(len(loader.solution_tokenizer.word_index) + 1)\n",
    "\n",
    "# Clears logs folder\n",
    "#!find logs -mindepth 1 -delete\n",
    "\n",
    "#tf.debugging.experimental.disable_dump_debug_info()\n",
    "\n",
    "#!kill 415\n",
    "\n",
    "#print(\"hello\")\n",
    "\n",
    "#print(tf.sysconfig.get_build_info())\n",
    "\n",
    "#tf.__version__\n",
    "#!tensorboard --version\n",
    "\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade tensorboard\n",
    "\n",
    "#f.config.run_functions_eagerly(False)\n",
    "#f.executing_eagerly()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
